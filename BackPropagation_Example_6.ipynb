{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제 6] accuracy() 메서드 리턴값 index_label_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneration:\n",
    "    \n",
    "    # target_position = 0 (첫번째열이 정답데이터), target_position=-1 (마지막열이 정답데이터)\n",
    "    def __init__(self, name, file_path, seperation_rate, target_position=-1):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.file_path = file_path\n",
    "        \n",
    "        self.seperation_rate = seperation_rate\n",
    "        \n",
    "        if (target_position == -1  or  target_position == 0):      \n",
    "            self.target_position = target_position\n",
    "        \n",
    "        else:\n",
    "            err_str = 'target_position must be -1 or 0'            \n",
    "            raise Exception(err_str)    \n",
    "            \n",
    "    \n",
    "    # print data target distribution \n",
    "    # str_of_kind : 'original data' or  'training data'  or  'test data'\n",
    "    def __display_target_distribution(self, data, str_of_kind='original data'):\n",
    "        \n",
    "        print('=======================================================================================================')\n",
    "        \n",
    "        target_data = data[ :, self.target_position ]\n",
    "        \n",
    "        # numpy.unique() 사용하여 loaded data target 분포 확인\n",
    "        unique, counts = np.unique(target_data, return_counts=True)\n",
    "\n",
    "        unique_target = []\n",
    "    \n",
    "        for index in range(len(unique)):\n",
    "        \n",
    "            print('[DataGeneration] unique number of ' + str_of_kind + ' = ', unique[index], ', count = ', counts[index])\n",
    "        \n",
    "            unique_target.append(unique[index])\n",
    "\n",
    "        for index in range(len(unique_target)):\n",
    "        \n",
    "            print('[DataGeneration] unique number of ' + str_of_kind + ' = ', unique_target[index], ', ratio = ', np.round(100 * counts[index] / (target_data.shape[0]), 2), ' %')\n",
    "    \n",
    "        print('=======================================================================================================')\n",
    "        \n",
    "        \n",
    "    # numpy.random.shuffle()  이용하여 training_data / test_data 생성\n",
    "    def generate(self):\n",
    "    \n",
    "        # 데이터 불러오기, 파일이 없는 경우 exception 발생\n",
    "\n",
    "        try:\n",
    "            loaded_data = np.loadtxt(self.file_path, delimiter=',', dtype=np.float32)\n",
    "            \n",
    "        except Exception as err:\n",
    "            print('[DataGeneration::generate()]  ', str(err))\n",
    "            raise Exception(str(err))\n",
    "\n",
    "        print(\"[DataGeneration]  loaded_data.shape = \", loaded_data.shape)\n",
    "            \n",
    "        # print the target distribution of original data \n",
    "        \n",
    "        self.__display_target_distribution(loaded_data, 'original data')\n",
    "        \n",
    "        \n",
    "        # 분리비율에 맞게 테스트데이터로 분리\n",
    "        total_data_num = len(loaded_data)\n",
    "        test_data_num = int(len(loaded_data) * self.seperation_rate)\n",
    "\n",
    "        # numpy.random.shuffle 을 이용하여 랜덤하게 데이터 섞기\n",
    "        np.random.shuffle(loaded_data)\n",
    "        \n",
    "        # test_data 는 0 : test_data_num\n",
    "        \n",
    "        \n",
    "        test_data = loaded_data[ 0:test_data_num ]\n",
    "\n",
    "        # training_data 는 test_data_num 부터 끝까지 \n",
    "        training_data = loaded_data[ test_data_num: ]\n",
    "\n",
    "        # display target distribution of generated data \n",
    "        \n",
    "        self.__display_target_distribution(training_data, 'training data')\n",
    "        \n",
    "        self.__display_target_distribution(test_data, 'test data')\n",
    "        \n",
    "        return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime      # datetime.now() 를 이용하여 학습 경과 시간 측정\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        # 은닉층 가중치  W2 = (784 X 100) Xavier/He 방법으로 self.W2 가중치 초기화\n",
    "        self.W2 = np.random.randn(self.input_nodes, self.hidden_nodes) / np.sqrt(self.input_nodes/2)\n",
    "        self.b2 = np.random.rand(self.hidden_nodes)      \n",
    "        \n",
    "        # 출력층 가중치는 W3 = (100X10)  Xavier/He 방법으로 self.W3 가중치 초기화\n",
    "        self.W3 = np.random.randn(self.hidden_nodes, self.output_nodes) / np.sqrt(self.hidden_nodes/2)\n",
    "        self.b3 = np.random.rand(self.output_nodes)      \n",
    "                        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 정의 (모두 행렬로 표시)\n",
    "        self.Z3 = np.zeros([1,output_nodes])\n",
    "        self.A3 = np.zeros([1,output_nodes])\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 정의 (모두 행렬로 표시)\n",
    "        self.Z2 = np.zeros([1,hidden_nodes])\n",
    "        self.A2 = np.zeros([1,hidden_nodes])\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 정의 (모두 행렬로 표시)\n",
    "        self.Z1 = np.zeros([1,input_nodes])    \n",
    "        self.A1 = np.zeros([1,input_nodes])       \n",
    "        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def feed_forward(self):  \n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 계산\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산    \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        return  -np.sum( self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1 - self.A3)+delta ) )    \n",
    "    \n",
    "    def loss_val(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 계산\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산    \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        return  -np.sum( self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1 - self.A3)+delta ) )\n",
    "   \n",
    "    \n",
    "    # 정확도 측정함수 \n",
    "    def accuracy(self, test_input_data, test_target_data):\n",
    "        \n",
    "        matched_list = []\n",
    "        \n",
    "        # index_label_prediction 저장 list\n",
    "        temp_list = []\n",
    "        index_label_prediction_list = []\n",
    "        \n",
    "        \n",
    "        for index in range(len(test_input_data)):\n",
    "                        \n",
    "            label = int(test_target_data[index])\n",
    "                        \n",
    "            # one-hot encoding을 위한 데이터 정규화 (data normalize)\n",
    "            data = (test_input_data[index] / 255.0 * 0.99) + 0.01\n",
    "                  \n",
    "            # predict 를 위해서 vector 을 matrix 로 변환하여 인수로 넘겨줌\n",
    "            predicted_num = self.predict(np.array(data, ndmin=2)) \n",
    "        \n",
    "            if label == predicted_num:\n",
    "                matched_list.append(index)\n",
    "                \n",
    "            else:\n",
    "                # index_label_prediction 리스트 생성\n",
    "                temp_list.append(index)\n",
    "                temp_list.append(label)\n",
    "                temp_list.append(predicted_num)\n",
    "                \n",
    "                index_label_prediction_list.append(temp_list)\n",
    "                \n",
    "                temp_list = []    # temp_list 초기화 해주지 않으면 심각한 error 발생\n",
    "                \n",
    "        \n",
    "        accuracy_val = (len(matched_list)/(len(test_input_data)))\n",
    "        \n",
    "        return accuracy_val, index_label_prediction_list\n",
    "    \n",
    "    \n",
    "    def train(self, input_data, target_data):   # input_data : 784 개, target_data : 10개\n",
    "        \n",
    "        self.target_data = target_data    \n",
    "        self.input_data = input_data\n",
    "        \n",
    "        # 먼저 feed forward 를 통해서 최종 출력값과 이를 바탕으로 현재의 에러 값 계산\n",
    "        loss_val = self.feed_forward()\n",
    "        \n",
    "        # 출력층 loss 인 loss_3 구함\n",
    "        loss_3 = (self.A3-self.target_data) * self.A3 * (1-self.A3)\n",
    "        \n",
    "        # 출력층 가중치 W3, 출력층 바이어스 b3 업데이트\n",
    "        self.W3 = self.W3 - self.learning_rate * np.dot(self.A2.T, loss_3)   \n",
    "        \n",
    "        self.b3 = self.b3 - self.learning_rate * loss_3  \n",
    "        \n",
    "        # 은닉층 loss 인 loss_2 구함        \n",
    "        loss_2 = np.dot(loss_3, self.W3.T) * self.A2 * (1-self.A2)\n",
    "        \n",
    "        # 은닉층 가중치 W2, 은닉층 바이어스 b2 업데이트\n",
    "        self.W2 = self.W2 - self.learning_rate * np.dot(self.A1.T, loss_2)   \n",
    "        \n",
    "        self.b2 = self.b2 - self.learning_rate * loss_2\n",
    "        \n",
    "    def predict(self, input_data):        # input_data 는 행렬로 입력됨 즉, (1, 784) shape 을 가짐        \n",
    "        \n",
    "        Z2 = np.dot(input_data, self.W2) + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        \n",
    "        Z3 = np.dot(A2, self.W3) + self.b3\n",
    "        A3 = sigmoid(Z3)\n",
    "        \n",
    "        predicted_num = np.argmax(A3)\n",
    "    \n",
    "        return predicted_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation 비율 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataGeneration]  loaded_data.shape =  (60000, 785)\n",
      "=======================================================================================================\n",
      "[DataGeneration] unique number of original data =  0.0 , count =  5923\n",
      "[DataGeneration] unique number of original data =  1.0 , count =  6742\n",
      "[DataGeneration] unique number of original data =  2.0 , count =  5958\n",
      "[DataGeneration] unique number of original data =  3.0 , count =  6131\n",
      "[DataGeneration] unique number of original data =  4.0 , count =  5842\n",
      "[DataGeneration] unique number of original data =  5.0 , count =  5421\n",
      "[DataGeneration] unique number of original data =  6.0 , count =  5918\n",
      "[DataGeneration] unique number of original data =  7.0 , count =  6265\n",
      "[DataGeneration] unique number of original data =  8.0 , count =  5851\n",
      "[DataGeneration] unique number of original data =  9.0 , count =  5949\n",
      "[DataGeneration] unique number of original data =  0.0 , ratio =  9.87  %\n",
      "[DataGeneration] unique number of original data =  1.0 , ratio =  11.24  %\n",
      "[DataGeneration] unique number of original data =  2.0 , ratio =  9.93  %\n",
      "[DataGeneration] unique number of original data =  3.0 , ratio =  10.22  %\n",
      "[DataGeneration] unique number of original data =  4.0 , ratio =  9.74  %\n",
      "[DataGeneration] unique number of original data =  5.0 , ratio =  9.04  %\n",
      "[DataGeneration] unique number of original data =  6.0 , ratio =  9.86  %\n",
      "[DataGeneration] unique number of original data =  7.0 , ratio =  10.44  %\n",
      "[DataGeneration] unique number of original data =  8.0 , ratio =  9.75  %\n",
      "[DataGeneration] unique number of original data =  9.0 , ratio =  9.91  %\n",
      "=======================================================================================================\n",
      "=======================================================================================================\n",
      "[DataGeneration] unique number of training data =  0.0 , count =  4097\n",
      "[DataGeneration] unique number of training data =  1.0 , count =  4707\n",
      "[DataGeneration] unique number of training data =  2.0 , count =  4129\n",
      "[DataGeneration] unique number of training data =  3.0 , count =  4308\n",
      "[DataGeneration] unique number of training data =  4.0 , count =  4124\n",
      "[DataGeneration] unique number of training data =  5.0 , count =  3804\n",
      "[DataGeneration] unique number of training data =  6.0 , count =  4122\n",
      "[DataGeneration] unique number of training data =  7.0 , count =  4458\n",
      "[DataGeneration] unique number of training data =  8.0 , count =  4114\n",
      "[DataGeneration] unique number of training data =  9.0 , count =  4137\n",
      "[DataGeneration] unique number of training data =  0.0 , ratio =  9.75  %\n",
      "[DataGeneration] unique number of training data =  1.0 , ratio =  11.21  %\n",
      "[DataGeneration] unique number of training data =  2.0 , ratio =  9.83  %\n",
      "[DataGeneration] unique number of training data =  3.0 , ratio =  10.26  %\n",
      "[DataGeneration] unique number of training data =  4.0 , ratio =  9.82  %\n",
      "[DataGeneration] unique number of training data =  5.0 , ratio =  9.06  %\n",
      "[DataGeneration] unique number of training data =  6.0 , ratio =  9.81  %\n",
      "[DataGeneration] unique number of training data =  7.0 , ratio =  10.61  %\n",
      "[DataGeneration] unique number of training data =  8.0 , ratio =  9.8  %\n",
      "[DataGeneration] unique number of training data =  9.0 , ratio =  9.85  %\n",
      "=======================================================================================================\n",
      "=======================================================================================================\n",
      "[DataGeneration] unique number of test data =  0.0 , count =  1826\n",
      "[DataGeneration] unique number of test data =  1.0 , count =  2035\n",
      "[DataGeneration] unique number of test data =  2.0 , count =  1829\n",
      "[DataGeneration] unique number of test data =  3.0 , count =  1823\n",
      "[DataGeneration] unique number of test data =  4.0 , count =  1718\n",
      "[DataGeneration] unique number of test data =  5.0 , count =  1617\n",
      "[DataGeneration] unique number of test data =  6.0 , count =  1796\n",
      "[DataGeneration] unique number of test data =  7.0 , count =  1807\n",
      "[DataGeneration] unique number of test data =  8.0 , count =  1737\n",
      "[DataGeneration] unique number of test data =  9.0 , count =  1812\n",
      "[DataGeneration] unique number of test data =  0.0 , ratio =  10.14  %\n",
      "[DataGeneration] unique number of test data =  1.0 , ratio =  11.31  %\n",
      "[DataGeneration] unique number of test data =  2.0 , ratio =  10.16  %\n",
      "[DataGeneration] unique number of test data =  3.0 , ratio =  10.13  %\n",
      "[DataGeneration] unique number of test data =  4.0 , ratio =  9.54  %\n",
      "[DataGeneration] unique number of test data =  5.0 , ratio =  8.98  %\n",
      "[DataGeneration] unique number of test data =  6.0 , ratio =  9.98  %\n",
      "[DataGeneration] unique number of test data =  7.0 , ratio =  10.04  %\n",
      "[DataGeneration] unique number of test data =  8.0 , ratio =  9.65  %\n",
      "[DataGeneration] unique number of test data =  9.0 , ratio =  10.07  %\n",
      "=======================================================================================================\n",
      "training_data.shape =  (42000, 785)\n",
      "validation_data.shape =  (18000, 785)\n"
     ]
    }
   ],
   "source": [
    "# DataGeneration class 이용하여 training data , validation data 생성\n",
    "seperation_rate = 0.3  # training data 10 % 비율로 validation data 생성\n",
    "target_position = 0    # 정답은 첫번째 열\n",
    "\n",
    "try:\n",
    "    data_obj = DataGeneration('MNIST', './mnist_train.csv', seperation_rate, target_position)\n",
    "\n",
    "    (training_data, validation_data) = data_obj.generate()\n",
    "    \n",
    "    print(\"training_data.shape = \", training_data.shape)\n",
    "    print(\"validation_data.shape = \", validation_data.shape)\n",
    "\n",
    "except Exception as err:\n",
    "    print('Exception Occur !!')\n",
    "    print(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 은닉층 노드 100 개 인 경우의 MNIST 오차역전파 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  1 , step =  0 , current loss_val =  7.57996014296751\n",
      "epochs =  1 , step =  1000 , current loss_val =  2.7103593493328573\n",
      "epochs =  1 , step =  2000 , current loss_val =  1.2516490182023576\n",
      "epochs =  1 , step =  3000 , current loss_val =  0.8033651295185574\n",
      "epochs =  1 , step =  4000 , current loss_val =  1.6757753106844997\n",
      "epochs =  1 , step =  5000 , current loss_val =  0.7155484257053566\n",
      "epochs =  1 , step =  6000 , current loss_val =  1.2721317512203645\n",
      "epochs =  1 , step =  7000 , current loss_val =  0.8614335092761559\n",
      "epochs =  1 , step =  8000 , current loss_val =  0.7698840532809507\n",
      "epochs =  1 , step =  9000 , current loss_val =  0.6783756689494461\n",
      "epochs =  1 , step =  10000 , current loss_val =  0.7418811925901385\n",
      "epochs =  1 , step =  11000 , current loss_val =  0.6435482698889423\n",
      "epochs =  1 , step =  12000 , current loss_val =  0.7327635669437131\n",
      "epochs =  1 , step =  13000 , current loss_val =  0.7043273072521998\n",
      "epochs =  1 , step =  14000 , current loss_val =  0.9201150038908414\n",
      "epochs =  1 , step =  15000 , current loss_val =  1.1244258539920635\n",
      "epochs =  1 , step =  16000 , current loss_val =  0.7091915442656875\n",
      "epochs =  1 , step =  17000 , current loss_val =  5.317576172063148\n",
      "epochs =  1 , step =  18000 , current loss_val =  1.187995553342707\n",
      "epochs =  1 , step =  19000 , current loss_val =  0.7428159849593823\n",
      "epochs =  1 , step =  20000 , current loss_val =  1.4489181909136875\n",
      "epochs =  1 , step =  21000 , current loss_val =  0.7036329226590189\n",
      "epochs =  1 , step =  22000 , current loss_val =  0.6981976124476674\n",
      "epochs =  1 , step =  23000 , current loss_val =  1.008569215802921\n",
      "epochs =  1 , step =  24000 , current loss_val =  0.87533852516209\n",
      "epochs =  1 , step =  25000 , current loss_val =  0.7478328378555146\n",
      "epochs =  1 , step =  26000 , current loss_val =  0.7204158998312018\n",
      "epochs =  1 , step =  27000 , current loss_val =  0.7824170995121827\n",
      "epochs =  1 , step =  28000 , current loss_val =  0.886167909013557\n",
      "epochs =  1 , step =  29000 , current loss_val =  0.7655747798328578\n",
      "epochs =  1 , step =  30000 , current loss_val =  0.8136852721480647\n",
      "epochs =  1 , step =  31000 , current loss_val =  0.769430457097853\n",
      "epochs =  1 , step =  32000 , current loss_val =  0.7507256037044181\n",
      "epochs =  1 , step =  33000 , current loss_val =  0.7569426961734523\n",
      "epochs =  1 , step =  34000 , current loss_val =  0.7971375839393472\n",
      "epochs =  1 , step =  35000 , current loss_val =  0.7475191885387517\n",
      "epochs =  1 , step =  36000 , current loss_val =  0.8212972765424787\n",
      "epochs =  1 , step =  37000 , current loss_val =  0.8125339887405383\n",
      "epochs =  1 , step =  38000 , current loss_val =  1.0030752048933842\n",
      "epochs =  1 , step =  39000 , current loss_val =  0.8448127651311937\n",
      "epochs =  1 , step =  40000 , current loss_val =  0.7879763466260729\n",
      "epochs =  1 , step =  41000 , current loss_val =  0.7055353230315957\n",
      "\n",
      "current epochs =  1  , current training accuracy =  92.60000000000001  %\n",
      "current epochs =  1  , current validation accuracy =  91.8  %\n",
      "\n",
      "epochs =  2 , step =  0 , current loss_val =  0.8250765885846744\n",
      "epochs =  2 , step =  1000 , current loss_val =  0.8003890168120109\n",
      "epochs =  2 , step =  2000 , current loss_val =  0.7988409128486451\n",
      "epochs =  2 , step =  3000 , current loss_val =  0.8105329236255814\n",
      "epochs =  2 , step =  4000 , current loss_val =  0.9492973672371114\n",
      "epochs =  2 , step =  5000 , current loss_val =  0.7042734441989265\n",
      "epochs =  2 , step =  6000 , current loss_val =  0.9154503266794785\n",
      "epochs =  2 , step =  7000 , current loss_val =  0.7069108710732713\n",
      "epochs =  2 , step =  8000 , current loss_val =  0.7355378025341407\n",
      "epochs =  2 , step =  9000 , current loss_val =  0.8287809126431052\n",
      "epochs =  2 , step =  10000 , current loss_val =  0.8165545251099984\n",
      "epochs =  2 , step =  11000 , current loss_val =  0.8128129314688272\n",
      "epochs =  2 , step =  12000 , current loss_val =  0.8282629490310809\n",
      "epochs =  2 , step =  13000 , current loss_val =  0.7482606845147338\n",
      "epochs =  2 , step =  14000 , current loss_val =  0.9183992239110309\n",
      "epochs =  2 , step =  15000 , current loss_val =  1.1433445106782367\n",
      "epochs =  2 , step =  16000 , current loss_val =  0.8203460921490053\n",
      "epochs =  2 , step =  17000 , current loss_val =  5.546660518397657\n",
      "epochs =  2 , step =  18000 , current loss_val =  1.0399881106376152\n",
      "epochs =  2 , step =  19000 , current loss_val =  0.7240120099256875\n",
      "epochs =  2 , step =  20000 , current loss_val =  0.8911732764620188\n",
      "epochs =  2 , step =  21000 , current loss_val =  0.8026279571987416\n",
      "epochs =  2 , step =  22000 , current loss_val =  0.7360087348526605\n",
      "epochs =  2 , step =  23000 , current loss_val =  0.9856301087889605\n",
      "epochs =  2 , step =  24000 , current loss_val =  0.9562964089803417\n",
      "epochs =  2 , step =  25000 , current loss_val =  0.8400964530767772\n",
      "epochs =  2 , step =  26000 , current loss_val =  0.7733652147542762\n",
      "epochs =  2 , step =  27000 , current loss_val =  0.7539064805603413\n",
      "epochs =  2 , step =  28000 , current loss_val =  0.7744583475396638\n",
      "epochs =  2 , step =  29000 , current loss_val =  0.8103340052883761\n",
      "epochs =  2 , step =  30000 , current loss_val =  0.8420221739781129\n",
      "epochs =  2 , step =  31000 , current loss_val =  0.7700648207285153\n",
      "epochs =  2 , step =  32000 , current loss_val =  0.7724928199823831\n",
      "epochs =  2 , step =  33000 , current loss_val =  0.7693572398971209\n",
      "epochs =  2 , step =  34000 , current loss_val =  0.8739508554068014\n",
      "epochs =  2 , step =  35000 , current loss_val =  0.7919310721292392\n",
      "epochs =  2 , step =  36000 , current loss_val =  0.7484170770140431\n",
      "epochs =  2 , step =  37000 , current loss_val =  0.7980296442567836\n",
      "epochs =  2 , step =  38000 , current loss_val =  0.9944183624676128\n",
      "epochs =  2 , step =  39000 , current loss_val =  0.8738154435824428\n",
      "epochs =  2 , step =  40000 , current loss_val =  0.7865690826783347\n",
      "epochs =  2 , step =  41000 , current loss_val =  0.7584249007186332\n",
      "\n",
      "current epochs =  2  , current training accuracy =  94.5  %\n",
      "current epochs =  2  , current validation accuracy =  93.60000000000001  %\n",
      "\n",
      "epochs =  3 , step =  0 , current loss_val =  0.8843974144581047\n",
      "epochs =  3 , step =  1000 , current loss_val =  0.8139087145230037\n",
      "epochs =  3 , step =  2000 , current loss_val =  0.8537113163751887\n",
      "epochs =  3 , step =  3000 , current loss_val =  0.8642292154964119\n",
      "epochs =  3 , step =  4000 , current loss_val =  0.8631279588974619\n",
      "epochs =  3 , step =  5000 , current loss_val =  0.7273343534619802\n",
      "epochs =  3 , step =  6000 , current loss_val =  0.8107258653375932\n",
      "epochs =  3 , step =  7000 , current loss_val =  0.7253120284940366\n",
      "epochs =  3 , step =  8000 , current loss_val =  0.747993576236331\n",
      "epochs =  3 , step =  9000 , current loss_val =  0.8704881241107785\n",
      "epochs =  3 , step =  10000 , current loss_val =  0.8548592060072783\n",
      "epochs =  3 , step =  11000 , current loss_val =  0.8600025341597118\n",
      "epochs =  3 , step =  12000 , current loss_val =  0.8684068215611578\n",
      "epochs =  3 , step =  13000 , current loss_val =  0.7709378071319873\n",
      "epochs =  3 , step =  14000 , current loss_val =  0.9046044831149791\n",
      "epochs =  3 , step =  15000 , current loss_val =  0.9911848980927562\n",
      "epochs =  3 , step =  16000 , current loss_val =  0.8335255204532006\n",
      "epochs =  3 , step =  17000 , current loss_val =  5.646770181029753\n",
      "epochs =  3 , step =  18000 , current loss_val =  0.9878941803221498\n",
      "epochs =  3 , step =  19000 , current loss_val =  0.761714592344566\n",
      "epochs =  3 , step =  20000 , current loss_val =  0.8988409007121222\n",
      "epochs =  3 , step =  21000 , current loss_val =  0.8291398165792115\n",
      "epochs =  3 , step =  22000 , current loss_val =  0.7534412144213876\n",
      "epochs =  3 , step =  23000 , current loss_val =  0.9776898135590778\n",
      "epochs =  3 , step =  24000 , current loss_val =  0.9640557642331294\n",
      "epochs =  3 , step =  25000 , current loss_val =  0.875580102674111\n",
      "epochs =  3 , step =  26000 , current loss_val =  0.7924315748006042\n",
      "epochs =  3 , step =  27000 , current loss_val =  0.7724143360871023\n",
      "epochs =  3 , step =  28000 , current loss_val =  0.7548119987495938\n",
      "epochs =  3 , step =  29000 , current loss_val =  0.8032996244467866\n",
      "epochs =  3 , step =  30000 , current loss_val =  0.8541826535921995\n",
      "epochs =  3 , step =  31000 , current loss_val =  0.7991351371962913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  3 , step =  32000 , current loss_val =  0.7737014873046123\n",
      "epochs =  3 , step =  33000 , current loss_val =  0.7815977572464903\n",
      "epochs =  3 , step =  34000 , current loss_val =  0.8901725526888772\n",
      "epochs =  3 , step =  35000 , current loss_val =  0.7989378390841458\n",
      "epochs =  3 , step =  36000 , current loss_val =  0.7683525432584408\n",
      "epochs =  3 , step =  37000 , current loss_val =  0.8168130303154538\n",
      "epochs =  3 , step =  38000 , current loss_val =  0.9766118950367522\n",
      "epochs =  3 , step =  39000 , current loss_val =  0.8784041408751152\n",
      "epochs =  3 , step =  40000 , current loss_val =  0.7812532225465119\n",
      "epochs =  3 , step =  41000 , current loss_val =  0.7761121587374148\n",
      "\n",
      "current epochs =  3  , current training accuracy =  95.5  %\n",
      "current epochs =  3  , current validation accuracy =  94.6  %\n",
      "\n",
      "epochs =  4 , step =  0 , current loss_val =  0.9249081828691149\n",
      "epochs =  4 , step =  1000 , current loss_val =  0.8322995273085171\n",
      "epochs =  4 , step =  2000 , current loss_val =  0.8759910399843649\n",
      "epochs =  4 , step =  3000 , current loss_val =  0.8844641590827531\n",
      "epochs =  4 , step =  4000 , current loss_val =  0.8176737924527685\n",
      "epochs =  4 , step =  5000 , current loss_val =  0.7279840028393729\n",
      "epochs =  4 , step =  6000 , current loss_val =  0.790345664130664\n",
      "epochs =  4 , step =  7000 , current loss_val =  0.7531246721522927\n",
      "epochs =  4 , step =  8000 , current loss_val =  0.7604932446621079\n",
      "epochs =  4 , step =  9000 , current loss_val =  0.8833953314936404\n",
      "epochs =  4 , step =  10000 , current loss_val =  0.8710728421167849\n",
      "epochs =  4 , step =  11000 , current loss_val =  0.8796812399107986\n",
      "epochs =  4 , step =  12000 , current loss_val =  0.8917716157516412\n",
      "epochs =  4 , step =  13000 , current loss_val =  0.7768487451761532\n",
      "epochs =  4 , step =  14000 , current loss_val =  0.8763767037657728\n",
      "epochs =  4 , step =  15000 , current loss_val =  0.9274938180056103\n",
      "epochs =  4 , step =  16000 , current loss_val =  0.8433900910260278\n",
      "epochs =  4 , step =  17000 , current loss_val =  5.326588634058288\n",
      "epochs =  4 , step =  18000 , current loss_val =  0.9515853821457294\n",
      "epochs =  4 , step =  19000 , current loss_val =  0.7893564413769538\n",
      "epochs =  4 , step =  20000 , current loss_val =  0.8874581305937376\n",
      "epochs =  4 , step =  21000 , current loss_val =  0.8466618345149248\n",
      "epochs =  4 , step =  22000 , current loss_val =  0.7655414353419121\n",
      "epochs =  4 , step =  23000 , current loss_val =  0.9257299695295323\n",
      "epochs =  4 , step =  24000 , current loss_val =  0.9412336335112989\n",
      "epochs =  4 , step =  25000 , current loss_val =  0.8954273290905467\n",
      "epochs =  4 , step =  26000 , current loss_val =  0.7956723057676455\n",
      "epochs =  4 , step =  27000 , current loss_val =  0.7766400684527066\n",
      "epochs =  4 , step =  28000 , current loss_val =  0.7632454831048842\n",
      "epochs =  4 , step =  29000 , current loss_val =  0.8041702769254027\n",
      "epochs =  4 , step =  30000 , current loss_val =  0.8650587533158269\n",
      "epochs =  4 , step =  31000 , current loss_val =  0.8205166006417748\n",
      "epochs =  4 , step =  32000 , current loss_val =  0.7756301568735591\n",
      "epochs =  4 , step =  33000 , current loss_val =  0.7971821475513083\n",
      "epochs =  4 , step =  34000 , current loss_val =  0.8908005082522356\n",
      "epochs =  4 , step =  35000 , current loss_val =  0.8174889210060664\n",
      "epochs =  4 , step =  36000 , current loss_val =  0.7940134204967924\n",
      "epochs =  4 , step =  37000 , current loss_val =  0.8357118710438812\n",
      "epochs =  4 , step =  38000 , current loss_val =  0.9318215919539039\n",
      "epochs =  4 , step =  39000 , current loss_val =  0.8723123775202769\n",
      "epochs =  4 , step =  40000 , current loss_val =  0.801530364960383\n",
      "epochs =  4 , step =  41000 , current loss_val =  0.7832574160469415\n",
      "\n",
      "current epochs =  4  , current training accuracy =  96.3  %\n",
      "current epochs =  4  , current validation accuracy =  95.1  %\n",
      "\n",
      "epochs =  5 , step =  0 , current loss_val =  0.9548966178734388\n",
      "epochs =  5 , step =  1000 , current loss_val =  0.847953630820397\n",
      "epochs =  5 , step =  2000 , current loss_val =  0.8879402746435487\n",
      "epochs =  5 , step =  3000 , current loss_val =  0.8867827471892855\n",
      "epochs =  5 , step =  4000 , current loss_val =  0.8202016090799048\n",
      "epochs =  5 , step =  5000 , current loss_val =  0.7262351291601128\n",
      "epochs =  5 , step =  6000 , current loss_val =  0.8065644833842615\n",
      "epochs =  5 , step =  7000 , current loss_val =  0.7673720767206877\n",
      "epochs =  5 , step =  8000 , current loss_val =  0.7757926772044373\n",
      "epochs =  5 , step =  9000 , current loss_val =  0.8932677998855081\n",
      "epochs =  5 , step =  10000 , current loss_val =  0.881593986822648\n",
      "epochs =  5 , step =  11000 , current loss_val =  0.8908594786468143\n",
      "epochs =  5 , step =  12000 , current loss_val =  0.9036556070348626\n",
      "epochs =  5 , step =  13000 , current loss_val =  0.7799781774245743\n",
      "epochs =  5 , step =  14000 , current loss_val =  0.8586349273813517\n",
      "epochs =  5 , step =  15000 , current loss_val =  0.9073593459230945\n",
      "epochs =  5 , step =  16000 , current loss_val =  0.8478514720202388\n",
      "epochs =  5 , step =  17000 , current loss_val =  4.8027705172716715\n",
      "epochs =  5 , step =  18000 , current loss_val =  0.9263167009706665\n",
      "epochs =  5 , step =  19000 , current loss_val =  0.8080611570203983\n",
      "epochs =  5 , step =  20000 , current loss_val =  0.8722953331046042\n",
      "epochs =  5 , step =  21000 , current loss_val =  0.8630705404267787\n",
      "epochs =  5 , step =  22000 , current loss_val =  0.7742247030857129\n",
      "epochs =  5 , step =  23000 , current loss_val =  0.8862347759440224\n",
      "epochs =  5 , step =  24000 , current loss_val =  0.9263573584542241\n",
      "epochs =  5 , step =  25000 , current loss_val =  0.9155222206547545\n",
      "epochs =  5 , step =  26000 , current loss_val =  0.8027945370445712\n",
      "epochs =  5 , step =  27000 , current loss_val =  0.7894083991279385\n",
      "epochs =  5 , step =  28000 , current loss_val =  0.7776210587023099\n",
      "epochs =  5 , step =  29000 , current loss_val =  0.80912189278784\n",
      "epochs =  5 , step =  30000 , current loss_val =  0.87305232451958\n",
      "epochs =  5 , step =  31000 , current loss_val =  0.8422660338767504\n",
      "epochs =  5 , step =  32000 , current loss_val =  0.7761568658207945\n",
      "epochs =  5 , step =  33000 , current loss_val =  0.81200221192988\n",
      "epochs =  5 , step =  34000 , current loss_val =  0.8956564061664086\n",
      "epochs =  5 , step =  35000 , current loss_val =  0.8366770654779211\n",
      "epochs =  5 , step =  36000 , current loss_val =  0.8209585702386756\n",
      "epochs =  5 , step =  37000 , current loss_val =  0.8507022359874674\n",
      "epochs =  5 , step =  38000 , current loss_val =  0.9133117341756903\n",
      "epochs =  5 , step =  39000 , current loss_val =  0.8715050003493697\n",
      "epochs =  5 , step =  40000 , current loss_val =  0.8320913013790233\n",
      "epochs =  5 , step =  41000 , current loss_val =  0.7921287698934292\n",
      "\n",
      "current epochs =  5  , current training accuracy =  96.8  %\n",
      "current epochs =  5  , current validation accuracy =  95.6  %\n",
      "\n",
      "epochs =  6 , step =  0 , current loss_val =  0.9760195377357331\n",
      "epochs =  6 , step =  1000 , current loss_val =  0.8569014177659594\n",
      "epochs =  6 , step =  2000 , current loss_val =  0.8925883035923685\n",
      "epochs =  6 , step =  3000 , current loss_val =  0.8894086528938037\n",
      "epochs =  6 , step =  4000 , current loss_val =  0.8274573222361163\n",
      "epochs =  6 , step =  5000 , current loss_val =  0.7325808673656639\n",
      "epochs =  6 , step =  6000 , current loss_val =  0.8345381569214161\n",
      "epochs =  6 , step =  7000 , current loss_val =  0.7791105023577163\n",
      "epochs =  6 , step =  8000 , current loss_val =  0.7823356301624383\n",
      "epochs =  6 , step =  9000 , current loss_val =  0.9127501686236521\n",
      "epochs =  6 , step =  10000 , current loss_val =  0.8877508139900434\n",
      "epochs =  6 , step =  11000 , current loss_val =  0.8994230684418661\n",
      "epochs =  6 , step =  12000 , current loss_val =  0.913369862743228\n",
      "epochs =  6 , step =  13000 , current loss_val =  0.7933850745196828\n",
      "epochs =  6 , step =  14000 , current loss_val =  0.8466593069456849\n",
      "epochs =  6 , step =  15000 , current loss_val =  0.907033414901193\n",
      "epochs =  6 , step =  16000 , current loss_val =  0.8570611668398562\n",
      "epochs =  6 , step =  17000 , current loss_val =  3.8755745034120386\n",
      "epochs =  6 , step =  18000 , current loss_val =  0.9258729695754229\n",
      "epochs =  6 , step =  19000 , current loss_val =  0.8223927114511346\n",
      "epochs =  6 , step =  20000 , current loss_val =  0.8583387721733807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  6 , step =  21000 , current loss_val =  0.8803288126831903\n",
      "epochs =  6 , step =  22000 , current loss_val =  0.7842147072187665\n",
      "epochs =  6 , step =  23000 , current loss_val =  0.8678680953053917\n",
      "epochs =  6 , step =  24000 , current loss_val =  0.9150036178284497\n",
      "epochs =  6 , step =  25000 , current loss_val =  0.9325470323360505\n",
      "epochs =  6 , step =  26000 , current loss_val =  0.8164609827364189\n",
      "epochs =  6 , step =  27000 , current loss_val =  0.8023591436313473\n",
      "epochs =  6 , step =  28000 , current loss_val =  0.7937629001676594\n",
      "epochs =  6 , step =  29000 , current loss_val =  0.8151025906842118\n",
      "epochs =  6 , step =  30000 , current loss_val =  0.8770054553071592\n",
      "epochs =  6 , step =  31000 , current loss_val =  0.8603245294372199\n",
      "epochs =  6 , step =  32000 , current loss_val =  0.7835878546890183\n",
      "epochs =  6 , step =  33000 , current loss_val =  0.8209442311257173\n",
      "epochs =  6 , step =  34000 , current loss_val =  0.90768661743919\n",
      "epochs =  6 , step =  35000 , current loss_val =  0.8523618097436939\n",
      "epochs =  6 , step =  36000 , current loss_val =  0.8459606313174646\n",
      "epochs =  6 , step =  37000 , current loss_val =  0.8689946795664931\n",
      "epochs =  6 , step =  38000 , current loss_val =  0.9088115348640263\n",
      "epochs =  6 , step =  39000 , current loss_val =  0.8713827044380786\n",
      "epochs =  6 , step =  40000 , current loss_val =  0.8811842246903794\n",
      "epochs =  6 , step =  41000 , current loss_val =  0.80498565060994\n",
      "\n",
      "current epochs =  6  , current training accuracy =  97.2  %\n",
      "current epochs =  6  , current validation accuracy =  95.89999999999999  %\n",
      "\n",
      "epochs =  7 , step =  0 , current loss_val =  0.9906316297475964\n",
      "epochs =  7 , step =  1000 , current loss_val =  0.8627461729650212\n",
      "epochs =  7 , step =  2000 , current loss_val =  0.8999464583452849\n",
      "epochs =  7 , step =  3000 , current loss_val =  0.891445284422639\n",
      "epochs =  7 , step =  4000 , current loss_val =  0.8374128651058476\n",
      "epochs =  7 , step =  5000 , current loss_val =  0.7372616790420189\n",
      "epochs =  7 , step =  6000 , current loss_val =  0.8749300760334883\n",
      "epochs =  7 , step =  7000 , current loss_val =  0.7920421818773932\n",
      "epochs =  7 , step =  8000 , current loss_val =  0.7896506489288811\n",
      "epochs =  7 , step =  9000 , current loss_val =  0.9322840244513813\n",
      "epochs =  7 , step =  10000 , current loss_val =  0.9010645262796427\n",
      "epochs =  7 , step =  11000 , current loss_val =  0.9099081201142267\n",
      "epochs =  7 , step =  12000 , current loss_val =  0.924105401637402\n",
      "epochs =  7 , step =  13000 , current loss_val =  0.806823721835369\n",
      "epochs =  7 , step =  14000 , current loss_val =  0.8416638232046042\n",
      "epochs =  7 , step =  15000 , current loss_val =  0.9136731532815842\n",
      "epochs =  7 , step =  16000 , current loss_val =  0.8653158823502268\n",
      "epochs =  7 , step =  17000 , current loss_val =  2.7748846722990175\n",
      "epochs =  7 , step =  18000 , current loss_val =  0.9340005973797271\n",
      "epochs =  7 , step =  19000 , current loss_val =  0.8404186125696699\n",
      "epochs =  7 , step =  20000 , current loss_val =  0.863968969554375\n",
      "epochs =  7 , step =  21000 , current loss_val =  0.8998792626026901\n",
      "epochs =  7 , step =  22000 , current loss_val =  0.7926071880230529\n",
      "epochs =  7 , step =  23000 , current loss_val =  0.8579579736687917\n",
      "epochs =  7 , step =  24000 , current loss_val =  0.9078771131648513\n",
      "epochs =  7 , step =  25000 , current loss_val =  0.9466247694899264\n",
      "epochs =  7 , step =  26000 , current loss_val =  0.8292878890822943\n",
      "epochs =  7 , step =  27000 , current loss_val =  0.8133220934352334\n",
      "epochs =  7 , step =  28000 , current loss_val =  0.8086867446004465\n",
      "epochs =  7 , step =  29000 , current loss_val =  0.8211436976567417\n",
      "epochs =  7 , step =  30000 , current loss_val =  0.8835918293311086\n",
      "epochs =  7 , step =  31000 , current loss_val =  0.8752558038768397\n",
      "epochs =  7 , step =  32000 , current loss_val =  0.7964936417687747\n",
      "epochs =  7 , step =  33000 , current loss_val =  0.8281164895434322\n",
      "epochs =  7 , step =  34000 , current loss_val =  0.9211465585913617\n",
      "epochs =  7 , step =  35000 , current loss_val =  0.8706182427149433\n",
      "epochs =  7 , step =  36000 , current loss_val =  0.8722936310015256\n",
      "epochs =  7 , step =  37000 , current loss_val =  0.8890166655482536\n",
      "epochs =  7 , step =  38000 , current loss_val =  0.9052480742787785\n",
      "epochs =  7 , step =  39000 , current loss_val =  0.8716369437026334\n",
      "epochs =  7 , step =  40000 , current loss_val =  0.9015152867552898\n",
      "epochs =  7 , step =  41000 , current loss_val =  0.8231412134592659\n",
      "\n",
      "current epochs =  7  , current training accuracy =  97.5  %\n",
      "current epochs =  7  , current validation accuracy =  96.1  %\n",
      "\n",
      "epochs =  8 , step =  0 , current loss_val =  1.003275525134516\n",
      "epochs =  8 , step =  1000 , current loss_val =  0.8685574305157594\n",
      "epochs =  8 , step =  2000 , current loss_val =  0.9106849585604095\n",
      "epochs =  8 , step =  3000 , current loss_val =  0.8904899750008477\n",
      "epochs =  8 , step =  4000 , current loss_val =  0.8436427635399625\n",
      "epochs =  8 , step =  5000 , current loss_val =  0.7423618583304689\n",
      "epochs =  8 , step =  6000 , current loss_val =  0.9110313013570258\n",
      "epochs =  8 , step =  7000 , current loss_val =  0.8065124017803686\n",
      "epochs =  8 , step =  8000 , current loss_val =  0.8022702023545205\n",
      "epochs =  8 , step =  9000 , current loss_val =  0.9496057819760029\n",
      "epochs =  8 , step =  10000 , current loss_val =  0.9159552113946008\n",
      "epochs =  8 , step =  11000 , current loss_val =  0.9165332397783711\n",
      "epochs =  8 , step =  12000 , current loss_val =  0.9373141770495819\n",
      "epochs =  8 , step =  13000 , current loss_val =  0.8185515964869244\n",
      "epochs =  8 , step =  14000 , current loss_val =  0.845916323696222\n",
      "epochs =  8 , step =  15000 , current loss_val =  0.9173043892422217\n",
      "epochs =  8 , step =  16000 , current loss_val =  0.8708557832959484\n",
      "epochs =  8 , step =  17000 , current loss_val =  1.8524383326483354\n",
      "epochs =  8 , step =  18000 , current loss_val =  0.9364019520383713\n",
      "epochs =  8 , step =  19000 , current loss_val =  0.8546430123549998\n",
      "epochs =  8 , step =  20000 , current loss_val =  0.8939331368509178\n",
      "epochs =  8 , step =  21000 , current loss_val =  0.9174341058467894\n",
      "epochs =  8 , step =  22000 , current loss_val =  0.8002429458620336\n",
      "epochs =  8 , step =  23000 , current loss_val =  0.850980866114683\n",
      "epochs =  8 , step =  24000 , current loss_val =  0.9068914686937288\n",
      "epochs =  8 , step =  25000 , current loss_val =  0.9559163750901778\n",
      "epochs =  8 , step =  26000 , current loss_val =  0.8376741298009848\n",
      "epochs =  8 , step =  27000 , current loss_val =  0.8265943563996487\n",
      "epochs =  8 , step =  28000 , current loss_val =  0.8238387380889401\n",
      "epochs =  8 , step =  29000 , current loss_val =  0.8288809767942623\n",
      "epochs =  8 , step =  30000 , current loss_val =  0.8907960234837919\n",
      "epochs =  8 , step =  31000 , current loss_val =  0.8900289092490167\n",
      "epochs =  8 , step =  32000 , current loss_val =  0.8071299838316635\n",
      "epochs =  8 , step =  33000 , current loss_val =  0.8373098526858653\n",
      "epochs =  8 , step =  34000 , current loss_val =  0.9372202009853942\n",
      "epochs =  8 , step =  35000 , current loss_val =  0.8924247110098333\n",
      "epochs =  8 , step =  36000 , current loss_val =  0.8949145790841954\n",
      "epochs =  8 , step =  37000 , current loss_val =  0.9106080463042562\n",
      "epochs =  8 , step =  38000 , current loss_val =  0.9000072681644639\n",
      "epochs =  8 , step =  39000 , current loss_val =  0.8770909510426699\n",
      "epochs =  8 , step =  40000 , current loss_val =  0.9130363830945812\n",
      "epochs =  8 , step =  41000 , current loss_val =  0.842999550718101\n",
      "\n",
      "current epochs =  8  , current training accuracy =  97.8  %\n",
      "current epochs =  8  , current validation accuracy =  96.2  %\n",
      "\n",
      "epochs =  9 , step =  0 , current loss_val =  1.0177955313428908\n",
      "epochs =  9 , step =  1000 , current loss_val =  0.8778692266567552\n",
      "epochs =  9 , step =  2000 , current loss_val =  0.9201749752223539\n",
      "epochs =  9 , step =  3000 , current loss_val =  0.8882989074378619\n",
      "epochs =  9 , step =  4000 , current loss_val =  0.8522745125680368\n",
      "epochs =  9 , step =  5000 , current loss_val =  0.7492858040062327\n",
      "epochs =  9 , step =  6000 , current loss_val =  0.9413158171760895\n",
      "epochs =  9 , step =  7000 , current loss_val =  0.8225726947951228\n",
      "epochs =  9 , step =  8000 , current loss_val =  0.8177989318956667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  9 , step =  9000 , current loss_val =  0.9677471665138384\n",
      "epochs =  9 , step =  10000 , current loss_val =  0.9278177262695974\n",
      "epochs =  9 , step =  11000 , current loss_val =  0.9253357684399897\n",
      "epochs =  9 , step =  12000 , current loss_val =  0.9553593073153127\n",
      "epochs =  9 , step =  13000 , current loss_val =  0.8310296897041536\n",
      "epochs =  9 , step =  14000 , current loss_val =  0.8596784883721118\n",
      "epochs =  9 , step =  15000 , current loss_val =  0.9146868662967278\n",
      "epochs =  9 , step =  16000 , current loss_val =  0.8825580283390196\n",
      "epochs =  9 , step =  17000 , current loss_val =  1.3036367850554156\n",
      "epochs =  9 , step =  18000 , current loss_val =  0.9300352485805087\n",
      "epochs =  9 , step =  19000 , current loss_val =  0.8660561104402211\n",
      "epochs =  9 , step =  20000 , current loss_val =  0.9195637437483252\n",
      "epochs =  9 , step =  21000 , current loss_val =  0.9344121703454173\n",
      "epochs =  9 , step =  22000 , current loss_val =  0.808858476868522\n",
      "epochs =  9 , step =  23000 , current loss_val =  0.8463597511544062\n",
      "epochs =  9 , step =  24000 , current loss_val =  0.9078136785949233\n",
      "epochs =  9 , step =  25000 , current loss_val =  0.9644728791700663\n",
      "epochs =  9 , step =  26000 , current loss_val =  0.8452836366763032\n",
      "epochs =  9 , step =  27000 , current loss_val =  0.8433461345053995\n",
      "epochs =  9 , step =  28000 , current loss_val =  0.8366313921548232\n",
      "epochs =  9 , step =  29000 , current loss_val =  0.8355999355710306\n",
      "epochs =  9 , step =  30000 , current loss_val =  0.8970369131541411\n",
      "epochs =  9 , step =  31000 , current loss_val =  0.9051843573683505\n",
      "epochs =  9 , step =  32000 , current loss_val =  0.814126202493286\n",
      "epochs =  9 , step =  33000 , current loss_val =  0.8512258987761364\n",
      "epochs =  9 , step =  34000 , current loss_val =  0.952318141194624\n",
      "epochs =  9 , step =  35000 , current loss_val =  0.9149937682415683\n",
      "epochs =  9 , step =  36000 , current loss_val =  0.9161174650148148\n",
      "epochs =  9 , step =  37000 , current loss_val =  0.9322976805102284\n",
      "epochs =  9 , step =  38000 , current loss_val =  0.8970480831467124\n",
      "epochs =  9 , step =  39000 , current loss_val =  0.8889797609757137\n",
      "epochs =  9 , step =  40000 , current loss_val =  0.9220131775905362\n",
      "epochs =  9 , step =  41000 , current loss_val =  0.861923886488382\n",
      "\n",
      "current epochs =  9  , current training accuracy =  97.89999999999999  %\n",
      "current epochs =  9  , current validation accuracy =  96.3  %\n",
      "\n",
      "epochs =  10 , step =  0 , current loss_val =  1.0363236426085796\n",
      "epochs =  10 , step =  1000 , current loss_val =  0.8924056727860851\n",
      "epochs =  10 , step =  2000 , current loss_val =  0.9292807084988368\n",
      "epochs =  10 , step =  3000 , current loss_val =  0.89617146186104\n",
      "epochs =  10 , step =  4000 , current loss_val =  0.8612193374346946\n",
      "epochs =  10 , step =  5000 , current loss_val =  0.7574308172339708\n",
      "epochs =  10 , step =  6000 , current loss_val =  0.9696361175097339\n",
      "epochs =  10 , step =  7000 , current loss_val =  0.8389176656665435\n",
      "epochs =  10 , step =  8000 , current loss_val =  0.8320132061131029\n",
      "epochs =  10 , step =  9000 , current loss_val =  0.9811442241693966\n",
      "epochs =  10 , step =  10000 , current loss_val =  0.9362727416081883\n",
      "epochs =  10 , step =  11000 , current loss_val =  0.9369471133010537\n",
      "epochs =  10 , step =  12000 , current loss_val =  0.9756030033224388\n",
      "epochs =  10 , step =  13000 , current loss_val =  0.8442816707997767\n",
      "epochs =  10 , step =  14000 , current loss_val =  0.8748980375495177\n",
      "epochs =  10 , step =  15000 , current loss_val =  0.9170560811124168\n",
      "epochs =  10 , step =  16000 , current loss_val =  0.8961914135077909\n",
      "epochs =  10 , step =  17000 , current loss_val =  1.1011349734998095\n",
      "epochs =  10 , step =  18000 , current loss_val =  0.9215925865532533\n",
      "epochs =  10 , step =  19000 , current loss_val =  0.8812685376882179\n",
      "epochs =  10 , step =  20000 , current loss_val =  0.9233483592349259\n",
      "epochs =  10 , step =  21000 , current loss_val =  0.9509123185207274\n",
      "epochs =  10 , step =  22000 , current loss_val =  0.8179730012141518\n",
      "epochs =  10 , step =  23000 , current loss_val =  0.8480498709251405\n",
      "epochs =  10 , step =  24000 , current loss_val =  0.9117548092833206\n",
      "epochs =  10 , step =  25000 , current loss_val =  0.9741923317515327\n",
      "epochs =  10 , step =  26000 , current loss_val =  0.8555425028435215\n",
      "epochs =  10 , step =  27000 , current loss_val =  0.8626196902346216\n",
      "epochs =  10 , step =  28000 , current loss_val =  0.8492322687619633\n",
      "epochs =  10 , step =  29000 , current loss_val =  0.8416019665581602\n",
      "epochs =  10 , step =  30000 , current loss_val =  0.9046465457341648\n",
      "epochs =  10 , step =  31000 , current loss_val =  0.9213703349532422\n",
      "epochs =  10 , step =  32000 , current loss_val =  0.820265301093531\n",
      "epochs =  10 , step =  33000 , current loss_val =  0.8694852415985427\n",
      "epochs =  10 , step =  34000 , current loss_val =  0.9653297530900924\n",
      "epochs =  10 , step =  35000 , current loss_val =  0.9333684644472084\n",
      "epochs =  10 , step =  36000 , current loss_val =  0.9357135580789375\n",
      "epochs =  10 , step =  37000 , current loss_val =  0.9520140316407896\n",
      "epochs =  10 , step =  38000 , current loss_val =  0.8956292435591225\n",
      "epochs =  10 , step =  39000 , current loss_val =  0.90278604519859\n",
      "epochs =  10 , step =  40000 , current loss_val =  0.9229951660434703\n",
      "epochs =  10 , step =  41000 , current loss_val =  0.8788402701491574\n",
      "\n",
      "current epochs =  10  , current training accuracy =  98.1  %\n",
      "current epochs =  10  , current validation accuracy =  96.3  %\n",
      "\n",
      "\n",
      "elapsed time =  0:06:50.890264\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter\n",
    "i_nodes = training_data.shape[1] - 1    # input nodes 개수\n",
    "h1_nodes = 100     # hidden 1 nodes\n",
    "o_nodes = 10       # output nodes\n",
    "lr = 0.1           # learning rate\n",
    "epochs = 10         # epochs\n",
    "\n",
    "# 손실함수 값을 저장할 list 생성\n",
    "loss_val_list = []\n",
    "\n",
    "# 정확도 저장 리스트\n",
    "training_accuracy_list = []\n",
    "validation_accuracy_list = []\n",
    "\n",
    "# 객체 생성\n",
    "nn = NeuralNetwork(i_nodes, h1_nodes, o_nodes, lr)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for step in range(len(training_data)):  # train\n",
    "    \n",
    "        # input_data, target_data normalize        \n",
    "        target_data = np.zeros(o_nodes) + 0.01    \n",
    "        target_data[int(training_data[step, 0])] = 0.99\n",
    "    \n",
    "        input_data = ((training_data[step, 1:] / 255.0) * 0.99) + 0.01\n",
    "    \n",
    "        nn.train( np.array(input_data, ndmin=2), np.array(target_data, ndmin=2) )\n",
    "    \n",
    "        if step % 1000 == 0:\n",
    "            print(\"epochs = \", i+1, \", step = \", step,  \", current loss_val = \", nn.loss_val())\n",
    "            \n",
    "        # 손실함수 값 저장 per step\n",
    "        loss_val_list.append(nn.loss_val())    \n",
    "        \n",
    "    # 정확도 계산 및 저장 per epochs\n",
    "    (training_accuracy, index_label_prediction_list) = nn.accuracy(training_data[:, 1:], training_data[:, 0])\n",
    "    (validation_accuracy, index_label_prediction_list) = nn.accuracy(validation_data[:, 1:], validation_data[:, 0])\n",
    "    \n",
    "    print('\\ncurrent epochs = ', i+1,' , current training accuracy = ', 100*np.round(training_accuracy,3), ' %')\n",
    "    print('current epochs = ', i+1,' , current validation accuracy = ', 100*np.round(validation_accuracy,3), ' %\\n')\n",
    "        \n",
    "    training_accuracy_list.append(training_accuracy)\n",
    "    validation_accuracy_list.append(validation_accuracy)\n",
    "        \n",
    "        \n",
    "end_time = datetime.now() \n",
    "print(\"\\nelapsed time = \", end_time - start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Training / Validation Accuracy Trend')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.plot(training_accuracy_list, label='training acc')\n",
    "plt.plot(validation_accuracy_list, label='validation acc')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data.shape =  (10000, 785)\n",
      "test_data[0,0] =  7.0 , len(test_data[0]) =  785\n",
      "Accuracy =  96.79  %\n"
     ]
    }
   ],
   "source": [
    "# 0~9 숫자 이미지가 784개의 숫자 (28X28) 로 구성되어 있는 test data 읽어옴\n",
    "\n",
    "try:\n",
    "    \n",
    "    test_data = np.loadtxt('./mnist_test.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "    test_input_data = test_data[ : , 1: ]\n",
    "    test_target_data = test_data[ : , 0 ]\n",
    "\n",
    "    print(\"test_data.shape = \", test_data.shape)\n",
    "    print(\"test_data[0,0] = \", test_data[0,0], \", len(test_data[0]) = \", len(test_data[0]))\n",
    "\n",
    "    # measure accuracy\n",
    "    (accuracy_ret, index_label_prediction_list) = nn.accuracy(test_input_data, test_target_data)   \n",
    "\n",
    "    print('Accuracy = ', np.round(100*accuracy_ret, 3), ' %')\n",
    "    \n",
    "except Exception as err:\n",
    "    \n",
    "    print('Exception occur !!')\n",
    "    print(str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9679\n",
      "false prediction data num =  321\n"
     ]
    }
   ],
   "source": [
    "# 총 오답 개수\n",
    "total_test_data_num = len(test_data)\n",
    "false_prediction_data_num = len(index_label_prediction_list)\n",
    "\n",
    "print('accuracy = ', (total_test_data_num - false_prediction_data_num) / total_test_data_num)\n",
    "print('false prediction data num = ', false_prediction_data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 5, 6], [33, 4, 0], [149, 2, 4], [241, 9, 5], [247, 4, 2], [259, 6, 0], [266, 8, 6], [290, 8, 4], [320, 9, 7], [321, 2, 7], [340, 5, 3], [352, 5, 0], [359, 9, 4], [391, 8, 2], [445, 6, 0], [448, 9, 8], [449, 3, 5], [479, 9, 5], [488, 9, 7], [495, 8, 2], [578, 3, 7], [582, 8, 2], [591, 8, 3], [619, 1, 8], [629, 2, 6], [659, 2, 7], [674, 5, 3], [691, 8, 4], [717, 0, 7], [720, 5, 8], [740, 4, 9], [760, 4, 9], [874, 9, 7], [882, 9, 7], [900, 1, 3], [924, 2, 7], [951, 5, 4], [965, 6, 0], [992, 9, 7], [999, 9, 7], [1014, 6, 5], [1044, 6, 2], [1062, 3, 7], [1107, 9, 3], [1112, 4, 6], [1166, 3, 2], [1192, 9, 4], [1204, 3, 7], [1226, 7, 2], [1232, 9, 4], [1242, 4, 9], [1247, 9, 3], [1299, 5, 7], [1319, 8, 3], [1326, 7, 1], [1328, 7, 4], [1378, 5, 6], [1393, 5, 3], [1414, 9, 7], [1494, 7, 0], [1500, 7, 1], [1522, 7, 9], [1530, 8, 7], [1549, 4, 6], [1553, 9, 3], [1609, 2, 6], [1621, 0, 6], [1678, 2, 0], [1681, 3, 7], [1709, 9, 3], [1717, 8, 0], [1751, 4, 2], [1754, 7, 2], [1790, 2, 7], [1850, 8, 7], [1878, 8, 3], [1901, 9, 4], [1909, 1, 7], [1940, 5, 0], [1952, 9, 5], [1955, 8, 2], [2016, 7, 2], [2018, 1, 7], [2035, 5, 3], [2044, 2, 7], [2053, 4, 9], [2093, 8, 1], [2098, 2, 0], [2109, 3, 7], [2118, 6, 0], [2129, 9, 2], [2130, 4, 9], [2135, 6, 1], [2182, 1, 2], [2185, 0, 5], [2186, 2, 3], [2224, 5, 6], [2266, 1, 6], [2272, 8, 0], [2291, 5, 7], [2293, 9, 6], [2299, 2, 7], [2369, 5, 6], [2387, 9, 1], [2395, 8, 6], [2406, 9, 1], [2414, 9, 4], [2422, 6, 4], [2426, 9, 4], [2488, 2, 4], [2526, 5, 3], [2573, 5, 8], [2598, 8, 2], [2607, 7, 2], [2648, 9, 0], [2654, 6, 1], [2720, 9, 4], [2739, 8, 4], [2769, 9, 7], [2797, 5, 7], [2810, 5, 3], [2812, 9, 4], [2863, 9, 4], [2877, 4, 7], [2896, 8, 0], [2921, 3, 2], [2925, 5, 0], [2927, 3, 2], [2939, 9, 7], [2945, 3, 7], [3005, 9, 1], [3060, 9, 7], [3073, 1, 2], [3117, 5, 9], [3157, 5, 7], [3206, 8, 3], [3240, 9, 8], [3284, 8, 7], [3336, 5, 7], [3422, 6, 0], [3475, 3, 7], [3503, 9, 1], [3520, 6, 4], [3549, 3, 2], [3558, 5, 0], [3559, 8, 5], [3567, 8, 5], [3597, 9, 3], [3662, 8, 6], [3674, 8, 7], [3751, 7, 2], [3757, 8, 3], [3767, 7, 2], [3776, 5, 8], [3780, 4, 6], [3806, 5, 7], [3811, 2, 4], [3817, 2, 4], [3818, 0, 6], [3853, 6, 0], [3855, 5, 0], [3871, 8, 3], [3893, 5, 6], [3902, 5, 3], [3906, 1, 3], [3926, 9, 3], [3941, 4, 6], [3946, 2, 8], [3951, 8, 2], [3985, 9, 4], [4000, 9, 4], [4065, 0, 6], [4075, 8, 3], [4078, 9, 3], [4093, 9, 4], [4140, 8, 2], [4163, 9, 0], [4176, 2, 6], [4199, 7, 9], [4201, 1, 7], [4207, 8, 2], [4212, 1, 2], [4224, 9, 7], [4248, 2, 8], [4289, 2, 7], [4300, 5, 9], [4306, 3, 7], [4314, 9, 7], [4344, 9, 2], [4355, 5, 9], [4359, 5, 7], [4369, 9, 4], [4374, 5, 6], [4405, 9, 4], [4419, 8, 4], [4425, 9, 4], [4435, 3, 7], [4437, 3, 2], [4477, 0, 6], [4497, 8, 7], [4505, 9, 7], [4534, 9, 7], [4571, 6, 2], [4575, 4, 2], [4601, 8, 4], [4615, 2, 4], [4731, 8, 7], [4761, 9, 7], [4763, 5, 6], [4807, 8, 3], [4814, 6, 0], [4823, 9, 4], [4874, 9, 0], [4876, 2, 4], [4879, 8, 6], [4880, 0, 8], [4886, 7, 1], [4956, 8, 4], [4966, 7, 9], [4990, 3, 2], [5046, 3, 0], [5140, 3, 4], [5176, 8, 4], [5331, 1, 6], [5457, 1, 8], [5495, 8, 3], [5523, 9, 7], [5547, 8, 0], [5593, 0, 6], [5611, 8, 6], [5642, 1, 5], [5719, 9, 7], [5734, 3, 7], [5735, 5, 6], [5749, 8, 6], [5752, 5, 6], [5757, 9, 7], [5842, 4, 7], [5857, 8, 3], [5887, 7, 2], [5888, 4, 0], [5913, 5, 3], [5936, 4, 9], [5937, 5, 3], [5955, 3, 8], [5972, 5, 3], [5973, 3, 8], [5982, 5, 3], [6045, 3, 9], [6059, 3, 8], [6071, 9, 3], [6166, 9, 3], [6172, 9, 0], [6505, 9, 0], [6555, 8, 7], [6560, 9, 7], [6568, 9, 4], [6571, 9, 7], [6574, 2, 6], [6597, 0, 7], [6598, 5, 3], [6625, 8, 7], [6632, 9, 5], [6651, 0, 5], [6755, 8, 7], [6783, 1, 6], [7043, 9, 7], [7208, 8, 7], [7216, 0, 6], [7259, 8, 7], [7432, 7, 1], [7434, 4, 8], [7451, 5, 6], [7545, 8, 9], [7823, 8, 2], [7849, 3, 2], [7858, 3, 2], [7886, 2, 4], [7918, 5, 6], [7921, 8, 6], [8020, 1, 8], [8094, 2, 8], [8183, 8, 5], [8277, 3, 5], [8279, 8, 2], [8318, 2, 0], [8325, 0, 6], [8408, 8, 6], [8410, 8, 6], [8522, 8, 6], [8863, 5, 6], [9009, 7, 2], [9015, 7, 2], [9024, 7, 2], [9482, 5, 3], [9517, 9, 4], [9530, 9, 7], [9587, 9, 4], [9634, 0, 3], [9642, 9, 7], [9643, 1, 7], [9664, 2, 7], [9679, 6, 3], [9692, 9, 7], [9716, 2, 0], [9729, 5, 6], [9741, 9, 7], [9745, 4, 2], [9749, 5, 6], [9768, 2, 0], [9770, 5, 0], [9777, 5, 0], [9779, 2, 0], [9808, 9, 4], [9839, 2, 7], [9879, 0, 2], [9905, 3, 7], [9941, 5, 6], [9944, 3, 8], [9959, 8, 7], [9982, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "# index_label_prediction_list 확인\n",
    "print(index_label_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
