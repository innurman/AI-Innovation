{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_1.x_Example_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MGTTOtFTWTe",
        "colab_type": "text"
      },
      "source": [
        "### [예제 1] TensorFlow_1.x_Example_1\n",
        "#### diabetes.csv 에 대한 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rdI0UW_TLUR",
        "colab_type": "code",
        "outputId": "edb6e1a3-6230-4573-9666-cf78896edad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8oo0avzLILk",
        "colab_type": "text"
      },
      "source": [
        "Google Colab 에서 Google Drive Mount 하는 기본 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_FiibvqLSLR",
        "colab_type": "code",
        "outputId": "35d80c67-6570-49fc-9ee4-232fa2073f36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive   # colab 사용시 mount 하기 위한 용도. local 에서는 불필요\n",
        "\n",
        "drive.mount('/content/gdrive/')  # colab 사용시 mount 하기 위한 용도. local 에서는 불필요"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWevb_LcOdIN",
        "colab_type": "text"
      },
      "source": [
        "Google Drive Mount 시킨 후에, 작업 디렉토리로 이동"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzmhwEqfLb4P",
        "colab_type": "code",
        "outputId": "b7aac634-8df8-4d7b-9818-06c2a8d2a998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# TensorFlow 1.x working directory\n",
        "working_dir = 'tensorflow_1.x_working_dir'\n",
        "\n",
        "\n",
        "# Google Drive 에서 Colab Default Directory\n",
        "colab_default_dir = '/content/gdrive/My Drive/Colab Notebooks/'\n",
        "\n",
        "\n",
        "curr_dir = os.getcwd()  # save current dir\n",
        "\n",
        "try:\n",
        "\n",
        "    os.chdir(colab_default_dir)\n",
        "\n",
        "    if not os.path.exists(working_dir):\n",
        "\n",
        "        os.mkdir(working_dir)\n",
        "\n",
        "    os.chdir(working_dir)  # change working dir\n",
        "\n",
        "    print('current dir = ', os.getcwd())\n",
        "\n",
        "except Exception as err:\n",
        "\n",
        "    # 원래의 dir 로 복귀\n",
        "    os.chdir(curr_dir)\n",
        "    print(str(err))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current dir =  /content/gdrive/My Drive/Colab Notebooks/tensorflow_1.x_working_dir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVLDKSkKQR35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# DataGeneration class\n",
        "\n",
        "class DataGeneration:\n",
        "    \n",
        "    # target_position = 0 (첫번째열이 정답데이터), target_position=-1 (마지막열이 정답데이터)\n",
        "    def __init__(self, name, file_path, seperation_rate, target_position=-1):\n",
        "        \n",
        "        self.name = name\n",
        "        \n",
        "        self.file_path = file_path\n",
        "        \n",
        "        self.seperation_rate = seperation_rate\n",
        "        \n",
        "        if (target_position == -1  or  target_position == 0):      \n",
        "            self.target_position = target_position\n",
        "        \n",
        "        else:\n",
        "            err_str = 'target_position must be -1 or 0'            \n",
        "            raise Exception(err_str)    \n",
        "            \n",
        "    \n",
        "    # print data target distribution \n",
        "    # str_of_kind : 'original data' or  'training data'  or  'test data'\n",
        "    def __display_target_distribution(self, data, str_of_kind='original data'):\n",
        "        \n",
        "        print('=======================================================================================================')\n",
        "        \n",
        "        target_data = data[ :, self.target_position ]\n",
        "        \n",
        "        # numpy.unique() 사용하여 loaded data target 분포 확인\n",
        "        unique, counts = np.unique(target_data, return_counts=True)\n",
        "\n",
        "        unique_target = []\n",
        "    \n",
        "        for index in range(len(unique)):\n",
        "        \n",
        "            print('[DataGeneration] unique number of ' + str_of_kind + ' = ', unique[index], ', count = ', counts[index])\n",
        "        \n",
        "            unique_target.append(unique[index])\n",
        "\n",
        "        for index in range(len(unique_target)):\n",
        "        \n",
        "            print('[DataGeneration] unique number of ' + str_of_kind + ' = ', unique_target[index], ', ratio = ', np.round(100 * counts[index] / (target_data.shape[0]), 2), ' %')\n",
        "    \n",
        "        print('=======================================================================================================')\n",
        "        \n",
        "        \n",
        "    # numpy.random.shuffle()  이용하여 training_data / test_data 생성\n",
        "    def generate(self):\n",
        "    \n",
        "        # 데이터 불러오기, 파일이 없는 경우 exception 발생\n",
        "\n",
        "        try:\n",
        "            loaded_data = np.loadtxt(self.file_path, delimiter=',', dtype=np.float32)\n",
        "            \n",
        "        except Exception as err:\n",
        "            print('[DataGeneration::generate()]  ', str(err))\n",
        "            raise Exception(str(err))\n",
        "\n",
        "        print(\"[DataGeneration]  loaded_data.shape = \", loaded_data.shape)\n",
        "            \n",
        "        # print the target distribution of original data \n",
        "        \n",
        "        self.__display_target_distribution(loaded_data, 'original data')\n",
        "        \n",
        "        \n",
        "        # 분리비율에 맞게 테스트데이터로 분리\n",
        "        total_data_num = len(loaded_data)\n",
        "        test_data_num = int(len(loaded_data) * self.seperation_rate)\n",
        "\n",
        "        # numpy.random.shuffle 을 이용하여 랜덤하게 데이터 섞기\n",
        "        np.random.shuffle(loaded_data)\n",
        "        \n",
        "        # test_data 는 0 : test_data_num\n",
        "        \n",
        "        \n",
        "        test_data = loaded_data[ 0:test_data_num ]\n",
        "\n",
        "        # training_data 는 test_data_num 부터 끝까지 \n",
        "        training_data = loaded_data[ test_data_num: ]\n",
        "\n",
        "        # display target distribution of generated data \n",
        "        \n",
        "        self.__display_target_distribution(training_data, 'training data')\n",
        "        \n",
        "        self.__display_target_distribution(test_data, 'test data')\n",
        "        \n",
        "        return training_data, test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeRA9jq8Uw2T",
        "colab_type": "code",
        "outputId": "d5426f3f-9ec7-47e8-b206-0309ae1134ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "# DataGeneration 객체 생성\n",
        "seperation_rate = 0.3\n",
        "\n",
        "data_obj = DataGeneration('Diabetes', './diabetes.csv', seperation_rate)  \n",
        "\n",
        "# training_data, test_data 생성\n",
        "(training_data, test_data) = data_obj.generate()\n",
        "\n",
        "print(\"training_data.shape = \", training_data.shape)\n",
        "print(\"test_data.shape = \", test_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DataGeneration]  loaded_data.shape =  (759, 9)\n",
            "=======================================================================================================\n",
            "[DataGeneration] unique number of original data =  0.0 , count =  263\n",
            "[DataGeneration] unique number of original data =  1.0 , count =  496\n",
            "[DataGeneration] unique number of original data =  0.0 , ratio =  34.65  %\n",
            "[DataGeneration] unique number of original data =  1.0 , ratio =  65.35  %\n",
            "=======================================================================================================\n",
            "=======================================================================================================\n",
            "[DataGeneration] unique number of training data =  0.0 , count =  179\n",
            "[DataGeneration] unique number of training data =  1.0 , count =  353\n",
            "[DataGeneration] unique number of training data =  0.0 , ratio =  33.65  %\n",
            "[DataGeneration] unique number of training data =  1.0 , ratio =  66.35  %\n",
            "=======================================================================================================\n",
            "=======================================================================================================\n",
            "[DataGeneration] unique number of test data =  0.0 , count =  84\n",
            "[DataGeneration] unique number of test data =  1.0 , count =  143\n",
            "[DataGeneration] unique number of test data =  0.0 , ratio =  37.0  %\n",
            "[DataGeneration] unique number of test data =  1.0 , ratio =  63.0  %\n",
            "=======================================================================================================\n",
            "training_data.shape =  (532, 9)\n",
            "test_data.shape =  (227, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiaXiJKSU0Az",
        "colab_type": "code",
        "outputId": "831632b5-1709-4517-a370-943f244bedd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "training_x_data = training_data[ :, 0:-1]\n",
        "training_t_data = training_data[ :, [-1]]\n",
        "\n",
        "print(\"training_x_data.shape = \", training_x_data.shape)\n",
        "print(\"training_t_data.shape = \", training_t_data.shape)\n",
        "\n",
        "test_x_data = test_data[ :, 0:-1]\n",
        "test_t_data = test_data[ :, [-1]]\n",
        "\n",
        "print(\"test_x_data.shape = \", test_x_data.shape)\n",
        "print(\"test_x_data.shape = \", test_x_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_x_data.shape =  (532, 8)\n",
            "training_t_data.shape =  (532, 1)\n",
            "test_x_data.shape =  (227, 8)\n",
            "test_x_data.shape =  (227, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2XxjwxGU3ec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_nodes = training_x_data.shape[1]  \n",
        "hidden_nodes = 10\n",
        "output_nodes = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THxAOR79U6Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.placeholder(tf.float32, [None, 8])  # 8개 입력노드\n",
        "T = tf.placeholder(tf.float32, [None, 1])  # 1개 정답노드\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([input_nodes, hidden_nodes]))  # 8X10 은닉층 노드\n",
        "b2 = tf.Variable(tf.random_normal([hidden_nodes]))     # 10개 은닉층 바이어스 노드\n",
        "\n",
        "W3 = tf.Variable(tf.random_normal([hidden_nodes, output_nodes]))  # 10X1 출력층 노드\n",
        "b3 = tf.Variable(tf.random_normal([output_nodes]))     # 10개 출력층 바이어스 노드"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOKfmXPxVN7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Z2 = tf.matmul(X, W2) + b2  # 은닉층 선형회귀 값 Z2\n",
        "A2 = tf.sigmoid(Z2)         # 은닉층 sigmoid\n",
        "\n",
        "Z3 = tf.matmul(A2, W3) + b3 # 출력층 선형회귀 값 Z3\n",
        "y = A3 = tf.sigmoid(Z3)     # 은닉층 sigmoid\n",
        "\n",
        "# 손실함수는 Cross-Entropy \n",
        "loss = -tf.reduce_mean( T*tf.log(y) + (1-T)*tf.log(1-y) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGFf-jYgYKMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01    # 학습율\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "\n",
        "train = optimizer.minimize(loss)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m14yur1eYMzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 정확성 검사, True if y > 0.5 else False\n",
        "\n",
        "predicted = tf.cast(y > 0.5, dtype=tf.float32)  \n",
        "\n",
        "# predicted 와 T 같으면 True 를 리턴하므로 cast 에 의해서 1로 강제 변환, \n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, T), dtype=tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9DxyXd9YO-Z",
        "colab_type": "code",
        "outputId": "74f7dcbc-439c-4f57-b54b-b67c409019d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "source": [
        "with  tf.Session()  as sess:\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())  # 변수 노드(tf.Variable) 초기화\n",
        "\n",
        "    for step in range(20001):\n",
        "      \n",
        "        loss_val, _ = sess.run([loss, train], feed_dict={X: training_x_data, T: training_t_data})    \n",
        "        \n",
        "        if step % 500 == 0:\n",
        "            print(\"step = \", step, \", loss_val = \", loss_val)             \n",
        "    \n",
        "    # Accuracy 확인\n",
        "    y_val, predicted_val, accuracy_val = sess.run([y, predicted, accuracy], feed_dict={X: test_x_data, T: test_t_data})\n",
        "    \n",
        "    \n",
        "    print(\"\\ny_val.shape = \", y_val.shape, \", predicted_val = \", predicted_val.shape)    \n",
        "    print(\"\\nAccuracy = \", accuracy_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step =  0 , loss_val =  0.8927491\n",
            "step =  500 , loss_val =  0.62250704\n",
            "step =  1000 , loss_val =  0.6124952\n",
            "step =  1500 , loss_val =  0.6028277\n",
            "step =  2000 , loss_val =  0.5933171\n",
            "step =  2500 , loss_val =  0.5839061\n",
            "step =  3000 , loss_val =  0.5746094\n",
            "step =  3500 , loss_val =  0.56549376\n",
            "step =  4000 , loss_val =  0.55665714\n",
            "step =  4500 , loss_val =  0.54820615\n",
            "step =  5000 , loss_val =  0.5402369\n",
            "step =  5500 , loss_val =  0.5328204\n",
            "step =  6000 , loss_val =  0.52599764\n",
            "step =  6500 , loss_val =  0.5197797\n",
            "step =  7000 , loss_val =  0.5141533\n",
            "step =  7500 , loss_val =  0.50908846\n",
            "step =  8000 , loss_val =  0.5045447\n",
            "step =  8500 , loss_val =  0.5004774\n",
            "step =  9000 , loss_val =  0.4968416\n",
            "step =  9500 , loss_val =  0.49359405\n",
            "step =  10000 , loss_val =  0.49069473\n",
            "step =  10500 , loss_val =  0.48810703\n",
            "step =  11000 , loss_val =  0.48579803\n",
            "step =  11500 , loss_val =  0.48373803\n",
            "step =  12000 , loss_val =  0.4819004\n",
            "step =  12500 , loss_val =  0.48026103\n",
            "step =  13000 , loss_val =  0.4787984\n",
            "step =  13500 , loss_val =  0.47749308\n",
            "step =  14000 , loss_val =  0.4763275\n",
            "step =  14500 , loss_val =  0.47528583\n",
            "step =  15000 , loss_val =  0.47435403\n",
            "step =  15500 , loss_val =  0.47351938\n",
            "step =  16000 , loss_val =  0.47277045\n",
            "step =  16500 , loss_val =  0.47209725\n",
            "step =  17000 , loss_val =  0.47149065\n",
            "step =  17500 , loss_val =  0.47094283\n",
            "step =  18000 , loss_val =  0.47044656\n",
            "step =  18500 , loss_val =  0.4699958\n",
            "step =  19000 , loss_val =  0.4695848\n",
            "step =  19500 , loss_val =  0.46920893\n",
            "step =  20000 , loss_val =  0.46886396\n",
            "\n",
            "y_val.shape =  (227, 1) , predicted_val =  (227, 1)\n",
            "\n",
            "Accuracy =  0.72246695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxlMiBnBYRoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}