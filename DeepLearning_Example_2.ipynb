{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제 2] 은닉층 1개를 가지는 신경망에서 AND / OR / NAND / XOR 검증\n",
    "#### 은닉층 노드 1개여도 AND, OR, NAND 검증 OK, 그러나 XOR은 은닉층노드 1개로는 검증 Not OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# 수치미분 함수\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = float(tmp_val) - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "# sigmoid 함수\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward\n",
    "def feed_forward(xdata, tdata):        # feed forward 를 통하여 손실함수(cross-entropy) 값 계산\n",
    "        \n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z2 = np.dot(xdata, W2) + b2  # 은닉층의 선형회귀 값\n",
    "    a2 = sigmoid(z2)                                  # 은닉층의 출력\n",
    "        \n",
    "    z3 = np.dot(a2, W3) + b3            # 출력층의 선형회귀 값\n",
    "    y = a3 = sigmoid(z3)                              # 출력층의 출력\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( tdata*np.log(y + delta) + (1-tdata)*np.log((1 - y)+delta ) )    \n",
    "\n",
    "\n",
    "\n",
    "# loss val\n",
    "def loss_val(xdata, tdata):        # feed forward 를 통하여 손실함수(cross-entropy) 값 계산\n",
    "        \n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z2 = np.dot(xdata, W2) + b2  # 은닉층의 선형회귀 값\n",
    "    a2 = sigmoid(z2)                                  # 은닉층의 출력\n",
    "        \n",
    "    z3 = np.dot(a2, W3) + b3            # 출력층의 선형회귀 값\n",
    "    y = a3 = sigmoid(z3)                              # 출력층의 출력\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( tdata*np.log(y + delta) + (1-tdata)*np.log((1 - y)+delta ) )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query, 즉 미래 값 예측 함수\n",
    "def predict(xdata):\n",
    "        \n",
    "    z2 = np.dot(xdata, W2) + b2         # 은닉층의 선형회귀 값\n",
    "    a2 = sigmoid(z2)                                  # 은닉층의 출력\n",
    "        \n",
    "    z3 = np.dot(a2, W3) + b3            # 출력층의 선형회귀 값\n",
    "    y = a3 = sigmoid(z3)                              # 출력층의 출력\n",
    "    \n",
    "    if y >= 0.5:\n",
    "        result = 1  # True\n",
    "    else:\n",
    "        result = 0  # False\n",
    "    \n",
    "    return y, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 입력데이터, 정답데이터 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and, or, nand, xor data\n",
    "xdata = np.array([ [0,0], [0,1], [1,0], [1,1] ])   \n",
    "\n",
    "and_tdata = np.array([0, 0, 0, 1]).reshape(4,1)\n",
    "or_tdata = np.array([0, 1, 1, 1]).reshape(4,1)\n",
    "nand_tdata = np.array([1, 1, 1, 0]).reshape(4,1)\n",
    "xor_tdata = np.array([0, 1, 1, 0]).reshape(4,1)\n",
    "\n",
    "# test data\n",
    "test_data = np.array([ [0,0], [0,1], [1,0], [1,1] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AND 학습 (은닉노드 10개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value =  14.764733659738724\n",
      "step =  0   , loss value =  9.534989800095264\n",
      "step =  500   , loss value =  0.177360347591258\n",
      "step =  1000   , loss value =  0.04275661817988801\n",
      "step =  1500   , loss value =  0.02184869437797192\n",
      "step =  2000   , loss value =  0.014189439289916003\n",
      "step =  2500   , loss value =  0.010341699719376908\n",
      "step =  3000   , loss value =  0.008062582645565258\n",
      "step =  3500   , loss value =  0.00656866068965488\n",
      "step =  4000   , loss value =  0.005519861146205362\n",
      "step =  4500   , loss value =  0.004746153213875925\n",
      "step =  5000   , loss value =  0.004153601120727699\n",
      "step =  5500   , loss value =  0.0036862982965532713\n",
      "step =  6000   , loss value =  0.003308994046768884\n",
      "step =  6500   , loss value =  0.002998412678784674\n",
      "step =  7000   , loss value =  0.002738594838904345\n",
      "step =  7500   , loss value =  0.0025182484680734097\n",
      "step =  8000   , loss value =  0.002329168594712055\n",
      "step =  8500   , loss value =  0.0021652559310325406\n",
      "step =  9000   , loss value =  0.0020218860137151415\n",
      "step =  9500   , loss value =  0.0018954914327822597\n",
      "step =  10000   , loss value =  0.0017832778925139688\n",
      "\n",
      "Elapsed Time =>  0:00:59.285327\n"
     ]
    }
   ],
   "source": [
    "# define neural network architecture\n",
    "input_nodes = 2   # 입력노드 2개\n",
    "hidden_nodes = 10  # 은닉노드 10개\n",
    "output_nodes= 1   # 출력노드 1개\n",
    "\n",
    "W2 = np.random.rand(input_nodes,hidden_nodes)  # 입력층-은닉층 가중치\n",
    "b2 = np.random.rand(hidden_nodes)    # 은닉층 바이어스\n",
    "\n",
    "W3 = np.random.rand(hidden_nodes,output_nodes)  # 은닉층-출력층 가중치\n",
    "b3 = np.random.rand(output_nodes)    # 출력층 바이어스\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# and verification\n",
    "f = lambda x : feed_forward(xdata, and_tdata)\n",
    "        \n",
    "print(\"Initial loss value = \", loss_val(xdata, and_tdata))\n",
    "     \n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in  range(10001):\n",
    "            \n",
    "    W2 -= learning_rate * numerical_derivative(f, W2)\n",
    "    \n",
    "    b2 -= learning_rate * numerical_derivative(f, b2)\n",
    "        \n",
    "    W3 -= learning_rate * numerical_derivative(f, W3)\n",
    "    \n",
    "    b3 -= learning_rate * numerical_derivative(f, b3)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"step = \", step, \"  , loss value = \", loss_val(xdata, and_tdata))\n",
    "        \n",
    "end_time = datetime.now()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_val [1.12238531e-07] , logical_val =  0\n",
      "real_val [0.0004548] , logical_val =  0\n",
      "real_val [0.00044904] , logical_val =  0\n",
      "real_val [0.99912086] , logical_val =  1\n"
     ]
    }
   ],
   "source": [
    "# and prediction\n",
    "\n",
    "for data in test_data:\n",
    "    (real_val, logical_val) = predict(data)\n",
    "    print(\"real_val\", real_val, \", logical_val = \", logical_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AND 학습 (은닉노드 1개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value =  3.568320510314834\n",
      "step =  0   , loss value =  3.2138166584596806\n",
      "step =  500   , loss value =  0.923386986727212\n",
      "step =  1000   , loss value =  0.13214360151648696\n",
      "step =  1500   , loss value =  0.06301785201244979\n",
      "step =  2000   , loss value =  0.040619555318930634\n",
      "step =  2500   , loss value =  0.029775643169068194\n",
      "step =  3000   , loss value =  0.023429509592948684\n",
      "step =  3500   , loss value =  0.019279864090369216\n",
      "step =  4000   , loss value =  0.016361243055643272\n",
      "step =  4500   , loss value =  0.014199755929085338\n",
      "step =  5000   , loss value =  0.012536260061157403\n",
      "step =  5500   , loss value =  0.011217369441572894\n",
      "step =  6000   , loss value =  0.010146619849851317\n",
      "step =  6500   , loss value =  0.009260363548616295\n",
      "step =  7000   , loss value =  0.008514938048992006\n",
      "step =  7500   , loss value =  0.00787940934855187\n",
      "step =  8000   , loss value =  0.007331258265018718\n",
      "step =  8500   , loss value =  0.006853707077611273\n",
      "step =  9000   , loss value =  0.00643400297132497\n",
      "step =  9500   , loss value =  0.006062281701603765\n",
      "step =  10000   , loss value =  0.005730795122215301\n",
      "\n",
      "Elapsed Time =>  0:00:07.692406\n"
     ]
    }
   ],
   "source": [
    "# define neural network architecture\n",
    "input_nodes = 2   # 입력노드 2개\n",
    "hidden_nodes = 1  # 은닉노드 1개\n",
    "output_nodes= 1   # 출력노드 1개\n",
    "\n",
    "W2 = np.random.rand(input_nodes,hidden_nodes)  # 입력층-은닉층 가중치\n",
    "b2 = np.random.rand(hidden_nodes)    # 은닉층 바이어스\n",
    "\n",
    "W3 = np.random.rand(hidden_nodes,output_nodes)  # 은닉층-출력층 가중치\n",
    "b3 = np.random.rand(output_nodes)    # 출력층 바이어스\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# and verification\n",
    "f = lambda x : feed_forward(xdata, and_tdata)\n",
    "        \n",
    "print(\"Initial loss value = \", loss_val(xdata, and_tdata))\n",
    "     \n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in  range(10001):\n",
    "            \n",
    "    W2 -= learning_rate * numerical_derivative(f, W2)\n",
    "    \n",
    "    b2 -= learning_rate * numerical_derivative(f, b2)\n",
    "        \n",
    "    W3 -= learning_rate * numerical_derivative(f, W3)\n",
    "    \n",
    "    b3 -= learning_rate * numerical_derivative(f, b3)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"step = \", step, \"  , loss value = \", loss_val(xdata, and_tdata))\n",
    "        \n",
    "end_time = datetime.now()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_val [0.00054295] , logical_val =  0\n",
      "real_val [0.0014125] , logical_val =  0\n",
      "real_val [0.0014125] , logical_val =  0\n",
      "real_val [0.99764168] , logical_val =  1\n"
     ]
    }
   ],
   "source": [
    "# and prediction\n",
    "\n",
    "for data in test_data:\n",
    "    (real_val, logical_val) = predict(data)\n",
    "    print(\"real_val\", real_val, \", logical_val = \", logical_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR 검증 (은닉노드 1개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value =  2.143547959042568\n",
      "step =  0   , loss value =  2.1418990541223355\n",
      "step =  400   , loss value =  0.2685529935714443\n",
      "step =  800   , loss value =  0.09629757769212956\n",
      "step =  1200   , loss value =  0.05682801267024123\n",
      "step =  1600   , loss value =  0.03998467382701928\n",
      "step =  2000   , loss value =  0.030742523070196533\n",
      "step =  2400   , loss value =  0.024929219221316266\n",
      "step =  2800   , loss value =  0.02094451032091159\n",
      "step =  3200   , loss value =  0.018046914542342786\n",
      "step =  3600   , loss value =  0.0158469482883049\n",
      "step =  4000   , loss value =  0.014120817720224768\n",
      "step =  4400   , loss value =  0.012730948318388908\n",
      "step =  4800   , loss value =  0.011588187629754058\n",
      "step =  5200   , loss value =  0.010632262165432366\n",
      "step =  5600   , loss value =  0.009820980083150998\n",
      "step =  6000   , loss value =  0.009123935002800105\n",
      "step =  6400   , loss value =  0.008518666163600492\n",
      "step =  6800   , loss value =  0.007988226228868056\n",
      "step =  7200   , loss value =  0.007519589925504073\n",
      "step =  7600   , loss value =  0.007102583166403021\n",
      "step =  8000   , loss value =  0.006729144491114695\n",
      "step =  8400   , loss value =  0.006392804530075642\n",
      "step =  8800   , loss value =  0.006088311988745598\n",
      "step =  9200   , loss value =  0.005811360230630832\n",
      "step =  9600   , loss value =  0.005558384271060673\n",
      "step =  10000   , loss value =  0.0053264079142154525\n",
      "\n",
      "Elapsed Time =>  0:00:08.396366\n"
     ]
    }
   ],
   "source": [
    "# define neural network architecture\n",
    "input_nodes = 2   # 입력노드 2개\n",
    "hidden_nodes = 1  # 은닉노드 1개\n",
    "output_nodes= 1   # 출력노드 1개\n",
    "\n",
    "W2 = np.random.rand(input_nodes,hidden_nodes)  # 입력층-은닉층 가중치\n",
    "b2 = np.random.rand(hidden_nodes)    # 은닉층 바이어스\n",
    "\n",
    "W3 = np.random.rand(hidden_nodes,output_nodes)  # 은닉층-출력층 가중치\n",
    "b3 = np.random.rand(output_nodes)    # 출력층 바이어스\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# or verification\n",
    "f = lambda x : feed_forward(xdata, or_tdata)\n",
    "        \n",
    "print(\"Initial loss value = \", loss_val(xdata, or_tdata))\n",
    "     \n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in  range(10001):\n",
    "            \n",
    "    W2 -= learning_rate * numerical_derivative(f, W2)\n",
    "    \n",
    "    b2 -= learning_rate * numerical_derivative(f, b2)\n",
    "        \n",
    "    W3 -= learning_rate * numerical_derivative(f, W3)\n",
    "    \n",
    "    b3 -= learning_rate * numerical_derivative(f, b3)\n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"step = \", step, \"  , loss value = \", loss_val(xdata, or_tdata))\n",
    "        \n",
    "end_time = datetime.now()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_val [0.00315601] , logical_val =  0\n",
      "real_val [0.99914315] , logical_val =  1\n",
      "real_val [0.99914311] , logical_val =  1\n",
      "real_val [0.99954876] , logical_val =  1\n"
     ]
    }
   ],
   "source": [
    "# or prediction\n",
    "\n",
    "for data in test_data:\n",
    "    (real_val, logical_val) = predict(data)\n",
    "    print(\"real_val\", real_val, \", logical_val = \", logical_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAND 검증 (은닉층 노드 1개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value =  2.2827256576148076\n",
      "step =  0   , loss value =  2.2776402026442994\n",
      "step =  500   , loss value =  0.3304234680999681\n",
      "step =  1000   , loss value =  0.09668409932017183\n",
      "step =  1500   , loss value =  0.05352231534629471\n",
      "step =  2000   , loss value =  0.03650376340490757\n",
      "step =  2500   , loss value =  0.027545201675704036\n",
      "step =  3000   , loss value =  0.022055195673184494\n",
      "step =  3500   , loss value =  0.01835957723759187\n",
      "step =  4000   , loss value =  0.015708019241136968\n",
      "step =  4500   , loss value =  0.01371570853653454\n",
      "step =  5000   , loss value =  0.012165518928073726\n",
      "step =  5500   , loss value =  0.010925880162822222\n",
      "step =  6000   , loss value =  0.009912523324257363\n",
      "step =  6500   , loss value =  0.009069029739143999\n",
      "step =  7000   , loss value =  0.008356231624120324\n",
      "step =  7500   , loss value =  0.007746102555879613\n",
      "step =  8000   , loss value =  0.007218068199464359\n",
      "step =  8500   , loss value =  0.006756689470905651\n",
      "step =  9000   , loss value =  0.006350158028683122\n",
      "step =  9500   , loss value =  0.005989290571673456\n",
      "step =  10000   , loss value =  0.005666839348784879\n",
      "\n",
      "Elapsed Time =>  0:00:08.273982\n"
     ]
    }
   ],
   "source": [
    "# define neural network architecture\n",
    "input_nodes = 2   # 입력노드 2개\n",
    "hidden_nodes = 1  # 은닉노드 1개\n",
    "output_nodes= 1   # 출력노드 1개\n",
    "\n",
    "W2 = np.random.rand(input_nodes,hidden_nodes)  # 입력층-은닉층 가중치\n",
    "b2 = np.random.rand(hidden_nodes)    # 은닉층 바이어스\n",
    "\n",
    "W3 = np.random.rand(hidden_nodes,output_nodes)  # 은닉층-출력층 가중치\n",
    "b3 = np.random.rand(output_nodes)    # 출력층 바이어스\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# nand verification\n",
    "f = lambda x : feed_forward(xdata, nand_tdata)\n",
    "        \n",
    "print(\"Initial loss value = \", loss_val(xdata, nand_tdata))\n",
    "     \n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in  range(10001):\n",
    "            \n",
    "    W2 -= learning_rate * numerical_derivative(f, W2)\n",
    "    \n",
    "    b2 -= learning_rate * numerical_derivative(f, b2)\n",
    "        \n",
    "    W3 -= learning_rate * numerical_derivative(f, W3)\n",
    "    \n",
    "    b3 -= learning_rate * numerical_derivative(f, b3)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"step = \", step, \"  , loss value = \", loss_val(xdata, nand_tdata))\n",
    "        \n",
    "end_time = datetime.now()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_val [0.99972562] , logical_val =  1\n",
      "real_val [0.99897815] , logical_val =  1\n",
      "real_val [0.99897815] , logical_val =  1\n",
      "real_val [0.00334249] , logical_val =  0\n"
     ]
    }
   ],
   "source": [
    "# nand prediction\n",
    "\n",
    "for data in test_data:\n",
    "    (real_val, logical_val) = predict(data)\n",
    "    print(\"real_val\", real_val, \", logical_val = \", logical_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### XOR 검증 (은닉층 노드 1개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value =  3.0495722319257235\n",
      "step =  0   , loss value =  2.9737390624926046\n",
      "step =  500   , loss value =  2.594516368856777\n",
      "step =  1000   , loss value =  2.0811710112502357\n",
      "step =  1500   , loss value =  1.9857602575547932\n",
      "step =  2000   , loss value =  1.956757679444613\n",
      "step =  2500   , loss value =  1.94332790082079\n",
      "step =  3000   , loss value =  1.9356937614226208\n",
      "step =  3500   , loss value =  1.930803275657285\n",
      "step =  4000   , loss value =  1.9274160904686772\n",
      "step =  4500   , loss value =  1.9249376655233856\n",
      "step =  5000   , loss value =  1.9230488212481378\n",
      "step =  5500   , loss value =  1.9215634123827376\n",
      "step =  6000   , loss value =  1.9203657928429578\n",
      "step =  6500   , loss value =  1.9193804390015963\n",
      "step =  7000   , loss value =  1.9185559958616394\n",
      "step =  7500   , loss value =  1.9178563569469842\n",
      "step =  8000   , loss value =  1.9172554140901845\n",
      "step =  8500   , loss value =  1.9167338317221159\n",
      "step =  9000   , loss value =  1.9162769914671123\n",
      "step =  9500   , loss value =  1.9158736407753119\n",
      "step =  10000   , loss value =  1.9155149799135036\n",
      "\n",
      "Elapsed Time =>  0:00:07.832118\n"
     ]
    }
   ],
   "source": [
    "# define neural network architecture\n",
    "input_nodes = 2   # 입력노드 2개\n",
    "hidden_nodes = 1  # 은닉노드 1개\n",
    "output_nodes= 1   # 출력노드 1개\n",
    "\n",
    "W2 = np.random.rand(input_nodes,hidden_nodes)  # 입력층-은닉층 가중치\n",
    "b2 = np.random.rand(hidden_nodes)    # 은닉층 바이어스\n",
    "\n",
    "W3 = np.random.rand(hidden_nodes,output_nodes)  # 은닉층-출력층 가중치\n",
    "b3 = np.random.rand(output_nodes)    # 출력층 바이어스\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# xor verification\n",
    "f = lambda x : feed_forward(xdata, xor_tdata)\n",
    "        \n",
    "print(\"Initial loss value = \", loss_val(xdata, xor_tdata))\n",
    "     \n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in  range(10001):\n",
    "            \n",
    "    W2 -= learning_rate * numerical_derivative(f, W2)\n",
    "    \n",
    "    b2 -= learning_rate * numerical_derivative(f, b2)\n",
    "        \n",
    "    W3 -= learning_rate * numerical_derivative(f, W3)\n",
    "    \n",
    "    b3 -= learning_rate * numerical_derivative(f, b3)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"step = \", step, \"  , loss value = \", loss_val(xdata, xor_tdata))\n",
    "        \n",
    "end_time = datetime.now()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_val [0.00309481] , logical_val =  0\n",
      "real_val [0.66567739] , logical_val =  1\n",
      "real_val [0.66567736] , logical_val =  1\n",
      "real_val [0.66663449] , logical_val =  1\n"
     ]
    }
   ],
   "source": [
    "# xor prediction\n",
    "\n",
    "for data in test_data:\n",
    "    (real_val, logical_val) = predict(data)\n",
    "    print(\"real_val\", real_val, \", logical_val = \", logical_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR 검증 (은닉층 노드 2개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value =  3.244230706153651\n",
      "step =  0   , loss value =  3.091458495467791\n",
      "step =  500   , loss value =  2.654172468901668\n",
      "step =  1000   , loss value =  1.6536382586745892\n",
      "step =  1500   , loss value =  1.48261034966289\n",
      "step =  2000   , loss value =  1.4419348180466032\n",
      "step =  2500   , loss value =  1.4247845507733687\n",
      "step =  3000   , loss value =  1.4154996788894147\n",
      "step =  3500   , loss value =  1.4097276434486996\n",
      "step =  4000   , loss value =  1.4058106627674585\n",
      "step =  4500   , loss value =  1.4029867482836806\n",
      "step =  5000   , loss value =  1.400858735705024\n",
      "step =  5500   , loss value =  1.3992000476887296\n",
      "step =  6000   , loss value =  1.397872308332742\n",
      "step =  6500   , loss value =  1.39678638090006\n",
      "step =  7000   , loss value =  1.3958823317918507\n",
      "step =  7500   , loss value =  1.395118417784484\n",
      "step =  8000   , loss value =  1.3944646952336326\n",
      "step =  8500   , loss value =  1.3938991401934508\n",
      "step =  9000   , loss value =  1.3934052013109208\n",
      "step =  9500   , loss value =  1.3929702051062753\n",
      "step =  10000   , loss value =  1.3925842868698766\n",
      "\n",
      "Elapsed Time =>  0:00:13.338419\n"
     ]
    }
   ],
   "source": [
    "# define neural network architecture\n",
    "input_nodes = 2   # 입력노드 2개\n",
    "hidden_nodes = 2  # 은닉노드 2개\n",
    "output_nodes= 1   # 출력노드 1개\n",
    "\n",
    "W2 = np.random.rand(input_nodes,hidden_nodes)  # 입력층-은닉층 가중치\n",
    "b2 = np.random.rand(hidden_nodes)    # 은닉층 바이어스\n",
    "\n",
    "W3 = np.random.rand(hidden_nodes,output_nodes)  # 은닉층-출력층 가중치\n",
    "b3 = np.random.rand(output_nodes)    # 출력층 바이어스\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# xor verification\n",
    "f = lambda x : feed_forward(xdata, xor_tdata)\n",
    "        \n",
    "print(\"Initial loss value = \", loss_val(xdata, xor_tdata))\n",
    "     \n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in  range(10001):\n",
    "            \n",
    "    W2 -= learning_rate * numerical_derivative(f, W2)\n",
    "    \n",
    "    b2 -= learning_rate * numerical_derivative(f, b2)\n",
    "        \n",
    "    W3 -= learning_rate * numerical_derivative(f, W3)\n",
    "    \n",
    "    b3 -= learning_rate * numerical_derivative(f, b3)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"step = \", step, \"  , loss value = \", loss_val(xdata, xor_tdata))\n",
    "        \n",
    "end_time = datetime.now()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_val [0.00160164] , logical_val =  0\n",
      "real_val [0.49935279] , logical_val =  0\n",
      "real_val [0.99820863] , logical_val =  1\n",
      "real_val [0.50079905] , logical_val =  1\n"
     ]
    }
   ],
   "source": [
    "# xor prediction\n",
    "\n",
    "for data in test_data:\n",
    "    (real_val, logical_val) = predict(data)\n",
    "    print(\"real_val\", real_val, \", logical_val = \", logical_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR 은닉층 노드 4개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value =  4.132379802367491\n",
      "step =  0   , loss value =  3.606345078174747\n",
      "step =  500   , loss value =  2.56461297625877\n",
      "step =  1000   , loss value =  1.1561440684108528\n",
      "step =  1500   , loss value =  0.23143652826951996\n",
      "step =  2000   , loss value =  0.10685171360222308\n",
      "step =  2500   , loss value =  0.06696657021258204\n",
      "step =  3000   , loss value =  0.0480549509096279\n",
      "step =  3500   , loss value =  0.037183795219550876\n",
      "step =  4000   , loss value =  0.030180841048085837\n",
      "step =  4500   , loss value =  0.025317569221718714\n",
      "step =  5000   , loss value =  0.02175537224024827\n",
      "step =  5500   , loss value =  0.019040405434202858\n",
      "step =  6000   , loss value =  0.016906511757714\n",
      "step =  6500   , loss value =  0.01518768675365499\n",
      "step =  7000   , loss value =  0.01377521720103371\n",
      "step =  7500   , loss value =  0.012595033770667702\n",
      "step =  8000   , loss value =  0.011594988974803613\n",
      "step =  8500   , loss value =  0.010737341559033633\n",
      "step =  9000   , loss value =  0.009994125393545529\n",
      "step =  9500   , loss value =  0.009344191631328998\n",
      "step =  10000   , loss value =  0.008771260354383335\n",
      "\n",
      "Elapsed Time =>  0:00:35.255851\n"
     ]
    }
   ],
   "source": [
    "# define neural network architecture\n",
    "input_nodes = 2   # 입력노드 2개\n",
    "hidden_nodes = 4  # 은닉노드 4개\n",
    "output_nodes= 1   # 출력노드 1개\n",
    "\n",
    "W2 = np.random.rand(input_nodes,hidden_nodes)  # 입력층-은닉층 가중치\n",
    "b2 = np.random.rand(hidden_nodes)    # 은닉층 바이어스\n",
    "\n",
    "W3 = np.random.rand(hidden_nodes,output_nodes)  # 은닉층-출력층 가중치\n",
    "b3 = np.random.rand(output_nodes)    # 출력층 바이어스\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# xor verification\n",
    "f = lambda x : feed_forward(xdata, xor_tdata)\n",
    "        \n",
    "print(\"Initial loss value = \", loss_val(xdata, xor_tdata))\n",
    "     \n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in  range(10001):\n",
    "            \n",
    "    W2 -= learning_rate * numerical_derivative(f, W2)\n",
    "    \n",
    "    b2 -= learning_rate * numerical_derivative(f, b2)\n",
    "        \n",
    "    W3 -= learning_rate * numerical_derivative(f, W3)\n",
    "    \n",
    "    b3 -= learning_rate * numerical_derivative(f, b3)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"step = \", step, \"  , loss value = \", loss_val(xdata, xor_tdata))\n",
    "        \n",
    "end_time = datetime.now()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_val [0.00279098] , logical_val =  0\n",
      "real_val [0.99801647] , logical_val =  1\n",
      "real_val [0.99790531] , logical_val =  1\n",
      "real_val [0.00189261] , logical_val =  0\n"
     ]
    }
   ],
   "source": [
    "# xor prediction\n",
    "\n",
    "for data in test_data:\n",
    "    (real_val, logical_val) = predict(data)\n",
    "    print(\"real_val\", real_val, \", logical_val = \", logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
