{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제 7] accuracy 메서드 리턴 index_label_prediction 에서 오답 이미지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneration:\n",
    "    \n",
    "    # target_position = 0 (첫번째열이 정답데이터), target_position=-1 (마지막열이 정답데이터)\n",
    "    def __init__(self, name, file_path, seperation_rate, target_position=-1):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.file_path = file_path\n",
    "        \n",
    "        self.seperation_rate = seperation_rate\n",
    "        \n",
    "        if (target_position == -1  or  target_position == 0):      \n",
    "            self.target_position = target_position\n",
    "        \n",
    "        else:\n",
    "            err_str = 'target_position must be -1 or 0'            \n",
    "            raise Exception(err_str)    \n",
    "            \n",
    "    \n",
    "    # print data target distribution \n",
    "    # str_of_kind : 'original data' or  'training data'  or  'test data'\n",
    "    def __display_target_distribution(self, data, str_of_kind='original data'):\n",
    "        \n",
    "        print('=======================================================================================================')\n",
    "        \n",
    "        target_data = data[ :, self.target_position ]\n",
    "        \n",
    "        # numpy.unique() 사용하여 loaded data target 분포 확인\n",
    "        unique, counts = np.unique(target_data, return_counts=True)\n",
    "\n",
    "        unique_target = []\n",
    "    \n",
    "        for index in range(len(unique)):\n",
    "        \n",
    "            print('[DataGeneration] unique number of ' + str_of_kind + ' = ', unique[index], ', count = ', counts[index])\n",
    "        \n",
    "            unique_target.append(unique[index])\n",
    "\n",
    "        for index in range(len(unique_target)):\n",
    "        \n",
    "            print('[DataGeneration] unique number of ' + str_of_kind + ' = ', unique_target[index], ', ratio = ', np.round(100 * counts[index] / (target_data.shape[0]), 2), ' %')\n",
    "    \n",
    "        print('=======================================================================================================')\n",
    "        \n",
    "        \n",
    "    # numpy.random.shuffle()  이용하여 training_data / test_data 생성\n",
    "    def generate(self):\n",
    "    \n",
    "        # 데이터 불러오기, 파일이 없는 경우 exception 발생\n",
    "\n",
    "        try:\n",
    "            loaded_data = np.loadtxt(self.file_path, delimiter=',', dtype=np.float32)\n",
    "            \n",
    "        except Exception as err:\n",
    "            print('[DataGeneration::generate()]  ', str(err))\n",
    "            raise Exception(str(err))\n",
    "\n",
    "        print(\"[DataGeneration]  loaded_data.shape = \", loaded_data.shape)\n",
    "            \n",
    "        # print the target distribution of original data \n",
    "        \n",
    "        self.__display_target_distribution(loaded_data, 'original data')\n",
    "        \n",
    "        \n",
    "        # 분리비율에 맞게 테스트데이터로 분리\n",
    "        total_data_num = len(loaded_data)\n",
    "        test_data_num = int(len(loaded_data) * self.seperation_rate)\n",
    "\n",
    "        # numpy.random.shuffle 을 이용하여 랜덤하게 데이터 섞기\n",
    "        np.random.shuffle(loaded_data)\n",
    "        \n",
    "        # test_data 는 0 : test_data_num\n",
    "        \n",
    "        \n",
    "        test_data = loaded_data[ 0:test_data_num ]\n",
    "\n",
    "        # training_data 는 test_data_num 부터 끝까지 \n",
    "        training_data = loaded_data[ test_data_num: ]\n",
    "\n",
    "        # display target distribution of generated data \n",
    "        \n",
    "        self.__display_target_distribution(training_data, 'training data')\n",
    "        \n",
    "        self.__display_target_distribution(test_data, 'test data')\n",
    "        \n",
    "        return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime      # datetime.now() 를 이용하여 학습 경과 시간 측정\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        # 은닉층 가중치  W2 = (784 X 100) Xavier/He 방법으로 self.W2 가중치 초기화\n",
    "        self.W2 = np.random.randn(self.input_nodes, self.hidden_nodes) / np.sqrt(self.input_nodes/2)\n",
    "        self.b2 = np.random.rand(self.hidden_nodes)      \n",
    "        \n",
    "        # 출력층 가중치는 W3 = (100X10)  Xavier/He 방법으로 self.W3 가중치 초기화\n",
    "        self.W3 = np.random.randn(self.hidden_nodes, self.output_nodes) / np.sqrt(self.hidden_nodes/2)\n",
    "        self.b3 = np.random.rand(self.output_nodes)      \n",
    "                        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 정의 (모두 행렬로 표시)\n",
    "        self.Z3 = np.zeros([1,output_nodes])\n",
    "        self.A3 = np.zeros([1,output_nodes])\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 정의 (모두 행렬로 표시)\n",
    "        self.Z2 = np.zeros([1,hidden_nodes])\n",
    "        self.A2 = np.zeros([1,hidden_nodes])\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 정의 (모두 행렬로 표시)\n",
    "        self.Z1 = np.zeros([1,input_nodes])    \n",
    "        self.A1 = np.zeros([1,input_nodes])       \n",
    "        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def feed_forward(self):  \n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 계산\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산    \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        return  -np.sum( self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1 - self.A3)+delta ) )    \n",
    "    \n",
    "    def loss_val(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 계산\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산    \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        return  -np.sum( self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1 - self.A3)+delta ) )\n",
    "   \n",
    "    \n",
    "    # 정확도 측정함수 \n",
    "    def accuracy(self, test_input_data, test_target_data):\n",
    "        \n",
    "        matched_list = []\n",
    "        \n",
    "        # index_label_prediction 저장 list\n",
    "        temp_list = []\n",
    "        index_label_prediction_list = []\n",
    "        \n",
    "        \n",
    "        for index in range(len(test_input_data)):\n",
    "                        \n",
    "            label = int(test_target_data[index])\n",
    "                        \n",
    "            # one-hot encoding을 위한 데이터 정규화 (data normalize)\n",
    "            data = (test_input_data[index] / 255.0 * 0.99) + 0.01\n",
    "                  \n",
    "            # predict 를 위해서 vector 을 matrix 로 변환하여 인수로 넘겨줌\n",
    "            predicted_num = self.predict(np.array(data, ndmin=2)) \n",
    "        \n",
    "            if label == predicted_num:\n",
    "                matched_list.append(index)\n",
    "                \n",
    "            else:\n",
    "                # index_label_prediction 리스트 생성\n",
    "                temp_list.append(index)\n",
    "                temp_list.append(label)\n",
    "                temp_list.append(predicted_num)\n",
    "                \n",
    "                index_label_prediction_list.append(temp_list)\n",
    "                \n",
    "                temp_list = []    # temp_list 초기화 해주지 않으면 심각한 error 발생\n",
    "                \n",
    "        \n",
    "        accuracy_val = (len(matched_list)/(len(test_input_data)))\n",
    "        \n",
    "        return accuracy_val, index_label_prediction_list\n",
    "    \n",
    "    \n",
    "    def train(self, input_data, target_data):   # input_data : 784 개, target_data : 10개\n",
    "        \n",
    "        self.target_data = target_data    \n",
    "        self.input_data = input_data\n",
    "        \n",
    "        # 먼저 feed forward 를 통해서 최종 출력값과 이를 바탕으로 현재의 에러 값 계산\n",
    "        loss_val = self.feed_forward()\n",
    "        \n",
    "        # 출력층 loss 인 loss_3 구함\n",
    "        loss_3 = (self.A3-self.target_data) * self.A3 * (1-self.A3)\n",
    "        \n",
    "        # 출력층 가중치 W3, 출력층 바이어스 b3 업데이트\n",
    "        self.W3 = self.W3 - self.learning_rate * np.dot(self.A2.T, loss_3)   \n",
    "        \n",
    "        self.b3 = self.b3 - self.learning_rate * loss_3  \n",
    "        \n",
    "        # 은닉층 loss 인 loss_2 구함        \n",
    "        loss_2 = np.dot(loss_3, self.W3.T) * self.A2 * (1-self.A2)\n",
    "        \n",
    "        # 은닉층 가중치 W2, 은닉층 바이어스 b2 업데이트\n",
    "        self.W2 = self.W2 - self.learning_rate * np.dot(self.A1.T, loss_2)   \n",
    "        \n",
    "        self.b2 = self.b2 - self.learning_rate * loss_2\n",
    "        \n",
    "    def predict(self, input_data):        # input_data 는 행렬로 입력됨 즉, (1, 784) shape 을 가짐        \n",
    "        \n",
    "        Z2 = np.dot(input_data, self.W2) + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        \n",
    "        Z3 = np.dot(A2, self.W3) + self.b3\n",
    "        A3 = sigmoid(Z3)\n",
    "        \n",
    "        predicted_num = np.argmax(A3)\n",
    "    \n",
    "        return predicted_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation 비율 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataGeneration]  loaded_data.shape =  (60000, 785)\n",
      "=======================================================================================================\n",
      "[DataGeneration] unique number of original data =  0.0 , count =  5923\n",
      "[DataGeneration] unique number of original data =  1.0 , count =  6742\n",
      "[DataGeneration] unique number of original data =  2.0 , count =  5958\n",
      "[DataGeneration] unique number of original data =  3.0 , count =  6131\n",
      "[DataGeneration] unique number of original data =  4.0 , count =  5842\n",
      "[DataGeneration] unique number of original data =  5.0 , count =  5421\n",
      "[DataGeneration] unique number of original data =  6.0 , count =  5918\n",
      "[DataGeneration] unique number of original data =  7.0 , count =  6265\n",
      "[DataGeneration] unique number of original data =  8.0 , count =  5851\n",
      "[DataGeneration] unique number of original data =  9.0 , count =  5949\n",
      "[DataGeneration] unique number of original data =  0.0 , ratio =  9.87  %\n",
      "[DataGeneration] unique number of original data =  1.0 , ratio =  11.24  %\n",
      "[DataGeneration] unique number of original data =  2.0 , ratio =  9.93  %\n",
      "[DataGeneration] unique number of original data =  3.0 , ratio =  10.22  %\n",
      "[DataGeneration] unique number of original data =  4.0 , ratio =  9.74  %\n",
      "[DataGeneration] unique number of original data =  5.0 , ratio =  9.04  %\n",
      "[DataGeneration] unique number of original data =  6.0 , ratio =  9.86  %\n",
      "[DataGeneration] unique number of original data =  7.0 , ratio =  10.44  %\n",
      "[DataGeneration] unique number of original data =  8.0 , ratio =  9.75  %\n",
      "[DataGeneration] unique number of original data =  9.0 , ratio =  9.91  %\n",
      "=======================================================================================================\n",
      "=======================================================================================================\n",
      "[DataGeneration] unique number of training data =  0.0 , count =  4745\n",
      "[DataGeneration] unique number of training data =  1.0 , count =  5426\n",
      "[DataGeneration] unique number of training data =  2.0 , count =  4764\n",
      "[DataGeneration] unique number of training data =  3.0 , count =  4857\n",
      "[DataGeneration] unique number of training data =  4.0 , count =  4693\n",
      "[DataGeneration] unique number of training data =  5.0 , count =  4366\n",
      "[DataGeneration] unique number of training data =  6.0 , count =  4786\n",
      "[DataGeneration] unique number of training data =  7.0 , count =  4974\n",
      "[DataGeneration] unique number of training data =  8.0 , count =  4671\n",
      "[DataGeneration] unique number of training data =  9.0 , count =  4718\n",
      "[DataGeneration] unique number of training data =  0.0 , ratio =  9.89  %\n",
      "[DataGeneration] unique number of training data =  1.0 , ratio =  11.3  %\n",
      "[DataGeneration] unique number of training data =  2.0 , ratio =  9.93  %\n",
      "[DataGeneration] unique number of training data =  3.0 , ratio =  10.12  %\n",
      "[DataGeneration] unique number of training data =  4.0 , ratio =  9.78  %\n",
      "[DataGeneration] unique number of training data =  5.0 , ratio =  9.1  %\n",
      "[DataGeneration] unique number of training data =  6.0 , ratio =  9.97  %\n",
      "[DataGeneration] unique number of training data =  7.0 , ratio =  10.36  %\n",
      "[DataGeneration] unique number of training data =  8.0 , ratio =  9.73  %\n",
      "[DataGeneration] unique number of training data =  9.0 , ratio =  9.83  %\n",
      "=======================================================================================================\n",
      "=======================================================================================================\n",
      "[DataGeneration] unique number of test data =  0.0 , count =  1178\n",
      "[DataGeneration] unique number of test data =  1.0 , count =  1316\n",
      "[DataGeneration] unique number of test data =  2.0 , count =  1194\n",
      "[DataGeneration] unique number of test data =  3.0 , count =  1274\n",
      "[DataGeneration] unique number of test data =  4.0 , count =  1149\n",
      "[DataGeneration] unique number of test data =  5.0 , count =  1055\n",
      "[DataGeneration] unique number of test data =  6.0 , count =  1132\n",
      "[DataGeneration] unique number of test data =  7.0 , count =  1291\n",
      "[DataGeneration] unique number of test data =  8.0 , count =  1180\n",
      "[DataGeneration] unique number of test data =  9.0 , count =  1231\n",
      "[DataGeneration] unique number of test data =  0.0 , ratio =  9.82  %\n",
      "[DataGeneration] unique number of test data =  1.0 , ratio =  10.97  %\n",
      "[DataGeneration] unique number of test data =  2.0 , ratio =  9.95  %\n",
      "[DataGeneration] unique number of test data =  3.0 , ratio =  10.62  %\n",
      "[DataGeneration] unique number of test data =  4.0 , ratio =  9.57  %\n",
      "[DataGeneration] unique number of test data =  5.0 , ratio =  8.79  %\n",
      "[DataGeneration] unique number of test data =  6.0 , ratio =  9.43  %\n",
      "[DataGeneration] unique number of test data =  7.0 , ratio =  10.76  %\n",
      "[DataGeneration] unique number of test data =  8.0 , ratio =  9.83  %\n",
      "[DataGeneration] unique number of test data =  9.0 , ratio =  10.26  %\n",
      "=======================================================================================================\n",
      "training_data.shape =  (48000, 785)\n",
      "validation_data.shape =  (12000, 785)\n"
     ]
    }
   ],
   "source": [
    "# DataGeneration class 이용하여 training data , validation data 생성\n",
    "seperation_rate = 0.2  # training data 10 % 비율로 validation data 생성\n",
    "target_position = 0    # 정답은 첫번째 열\n",
    "\n",
    "try:\n",
    "    data_obj = DataGeneration('MNIST', './mnist_train.csv', seperation_rate, target_position)\n",
    "\n",
    "    (training_data, validation_data) = data_obj.generate()\n",
    "    \n",
    "    print(\"training_data.shape = \", training_data.shape)\n",
    "    print(\"validation_data.shape = \", validation_data.shape)\n",
    "\n",
    "except Exception as err:\n",
    "    print('Exception Occur !!')\n",
    "    print(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 은닉층 노드 100 개 인 경우의 MNIST 오차역전파 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  1 , step =  0 , current loss_val =  6.561828476123224\n",
      "epochs =  1 , step =  1000 , current loss_val =  1.5230829379325017\n",
      "epochs =  1 , step =  2000 , current loss_val =  1.6186929577746532\n",
      "epochs =  1 , step =  3000 , current loss_val =  1.4418853930388202\n",
      "epochs =  1 , step =  4000 , current loss_val =  0.8113394329796562\n",
      "epochs =  1 , step =  5000 , current loss_val =  0.7901906642353481\n",
      "epochs =  1 , step =  6000 , current loss_val =  0.6925599757458182\n",
      "epochs =  1 , step =  7000 , current loss_val =  1.3647966082686398\n",
      "epochs =  1 , step =  8000 , current loss_val =  0.7557765849851777\n",
      "epochs =  1 , step =  9000 , current loss_val =  0.6242042489560111\n",
      "epochs =  1 , step =  10000 , current loss_val =  3.0661312243909733\n",
      "epochs =  1 , step =  11000 , current loss_val =  0.8695082923331289\n",
      "epochs =  1 , step =  12000 , current loss_val =  0.6861906749141311\n",
      "epochs =  1 , step =  13000 , current loss_val =  1.1233094954398986\n",
      "epochs =  1 , step =  14000 , current loss_val =  0.7646176396272787\n",
      "epochs =  1 , step =  15000 , current loss_val =  0.7003237608575014\n",
      "epochs =  1 , step =  16000 , current loss_val =  0.9409163939325524\n",
      "epochs =  1 , step =  17000 , current loss_val =  4.023783418823466\n",
      "epochs =  1 , step =  18000 , current loss_val =  0.7060526714547325\n",
      "epochs =  1 , step =  19000 , current loss_val =  0.8618495799694917\n",
      "epochs =  1 , step =  20000 , current loss_val =  0.6952490796274128\n",
      "epochs =  1 , step =  21000 , current loss_val =  0.8449743571306076\n",
      "epochs =  1 , step =  22000 , current loss_val =  0.8270297326093579\n",
      "epochs =  1 , step =  23000 , current loss_val =  0.8375926278020966\n",
      "epochs =  1 , step =  24000 , current loss_val =  1.09855883491596\n",
      "epochs =  1 , step =  25000 , current loss_val =  0.6878734178061344\n",
      "epochs =  1 , step =  26000 , current loss_val =  0.7546724761263927\n",
      "epochs =  1 , step =  27000 , current loss_val =  0.8229111857433724\n",
      "epochs =  1 , step =  28000 , current loss_val =  0.821785252259334\n",
      "epochs =  1 , step =  29000 , current loss_val =  0.8022472143695696\n",
      "epochs =  1 , step =  30000 , current loss_val =  0.7332488068565216\n",
      "epochs =  1 , step =  31000 , current loss_val =  0.7337050182465239\n",
      "epochs =  1 , step =  32000 , current loss_val =  0.7529667561342269\n",
      "epochs =  1 , step =  33000 , current loss_val =  0.7896428551406471\n",
      "epochs =  1 , step =  34000 , current loss_val =  0.8099355730032312\n",
      "epochs =  1 , step =  35000 , current loss_val =  1.039313871666907\n",
      "epochs =  1 , step =  36000 , current loss_val =  0.689090169897094\n",
      "epochs =  1 , step =  37000 , current loss_val =  0.8204665981027341\n",
      "epochs =  1 , step =  38000 , current loss_val =  0.7418247832324334\n",
      "epochs =  1 , step =  39000 , current loss_val =  0.722044409878859\n",
      "epochs =  1 , step =  40000 , current loss_val =  0.7993682361156471\n",
      "epochs =  1 , step =  41000 , current loss_val =  0.7834747464577065\n",
      "epochs =  1 , step =  42000 , current loss_val =  0.7113627509291404\n",
      "epochs =  1 , step =  43000 , current loss_val =  0.7756209555407465\n",
      "epochs =  1 , step =  44000 , current loss_val =  0.7274237488538686\n",
      "epochs =  1 , step =  45000 , current loss_val =  0.7969501249001303\n",
      "epochs =  1 , step =  46000 , current loss_val =  0.8882796478172039\n",
      "epochs =  1 , step =  47000 , current loss_val =  0.9174079201737007\n",
      "\n",
      "current epochs =  1  , current training accuracy =  93.5  %\n",
      "current epochs =  1  , current validation accuracy =  93.30000000000001  %\n",
      "\n",
      "epochs =  2 , step =  0 , current loss_val =  0.7993988552857281\n",
      "epochs =  2 , step =  1000 , current loss_val =  0.6901554833374799\n",
      "epochs =  2 , step =  2000 , current loss_val =  2.942902648438742\n",
      "epochs =  2 , step =  3000 , current loss_val =  0.7567337605497568\n",
      "epochs =  2 , step =  4000 , current loss_val =  0.8238099477486385\n",
      "epochs =  2 , step =  5000 , current loss_val =  0.7365925996829994\n",
      "epochs =  2 , step =  6000 , current loss_val =  0.7453505126276041\n",
      "epochs =  2 , step =  7000 , current loss_val =  0.9990653919103136\n",
      "epochs =  2 , step =  8000 , current loss_val =  0.7035171398478078\n",
      "epochs =  2 , step =  9000 , current loss_val =  0.8113155539852378\n",
      "epochs =  2 , step =  10000 , current loss_val =  1.0781504725303466\n",
      "epochs =  2 , step =  11000 , current loss_val =  0.7917243061808632\n",
      "epochs =  2 , step =  12000 , current loss_val =  0.7821133703079465\n",
      "epochs =  2 , step =  13000 , current loss_val =  0.9692568601804512\n",
      "epochs =  2 , step =  14000 , current loss_val =  0.7939760834353261\n",
      "epochs =  2 , step =  15000 , current loss_val =  0.7829436186701246\n",
      "epochs =  2 , step =  16000 , current loss_val =  0.7739407622232822\n",
      "epochs =  2 , step =  17000 , current loss_val =  1.805053854578623\n",
      "epochs =  2 , step =  18000 , current loss_val =  0.7653194873392548\n",
      "epochs =  2 , step =  19000 , current loss_val =  0.7980095956447993\n",
      "epochs =  2 , step =  20000 , current loss_val =  0.8414893735796394\n",
      "epochs =  2 , step =  21000 , current loss_val =  0.8532866755503016\n",
      "epochs =  2 , step =  22000 , current loss_val =  0.7834291167949764\n",
      "epochs =  2 , step =  23000 , current loss_val =  0.8719689785087477\n",
      "epochs =  2 , step =  24000 , current loss_val =  1.105502809069054\n",
      "epochs =  2 , step =  25000 , current loss_val =  0.7438715146206155\n",
      "epochs =  2 , step =  26000 , current loss_val =  0.8584106463841956\n",
      "epochs =  2 , step =  27000 , current loss_val =  0.7796301497038142\n",
      "epochs =  2 , step =  28000 , current loss_val =  0.8416796966346037\n",
      "epochs =  2 , step =  29000 , current loss_val =  0.7977528279509914\n",
      "epochs =  2 , step =  30000 , current loss_val =  0.7405401197762936\n",
      "epochs =  2 , step =  31000 , current loss_val =  0.7386692383880692\n",
      "epochs =  2 , step =  32000 , current loss_val =  0.7768993993200018\n",
      "epochs =  2 , step =  33000 , current loss_val =  0.8207219702106512\n",
      "epochs =  2 , step =  34000 , current loss_val =  0.8211010938301602\n",
      "epochs =  2 , step =  35000 , current loss_val =  0.9092988363751483\n",
      "epochs =  2 , step =  36000 , current loss_val =  0.7325717449491174\n",
      "epochs =  2 , step =  37000 , current loss_val =  0.7889048175065667\n",
      "epochs =  2 , step =  38000 , current loss_val =  0.8096571288464736\n",
      "epochs =  2 , step =  39000 , current loss_val =  0.7308433189145973\n",
      "epochs =  2 , step =  40000 , current loss_val =  0.8156040617804081\n",
      "epochs =  2 , step =  41000 , current loss_val =  0.8261457512138659\n",
      "epochs =  2 , step =  42000 , current loss_val =  0.7038716296231855\n",
      "epochs =  2 , step =  43000 , current loss_val =  0.8449889388338656\n",
      "epochs =  2 , step =  44000 , current loss_val =  0.7451981644700304\n",
      "epochs =  2 , step =  45000 , current loss_val =  0.9271865982965937\n",
      "epochs =  2 , step =  46000 , current loss_val =  0.8670452465016995\n",
      "epochs =  2 , step =  47000 , current loss_val =  0.8965235087289991\n",
      "\n",
      "current epochs =  2  , current training accuracy =  95.3  %\n",
      "current epochs =  2  , current validation accuracy =  94.69999999999999  %\n",
      "\n",
      "epochs =  3 , step =  0 , current loss_val =  0.7975572387919283\n",
      "epochs =  3 , step =  1000 , current loss_val =  0.7243772296802846\n",
      "epochs =  3 , step =  2000 , current loss_val =  3.620032370146811\n",
      "epochs =  3 , step =  3000 , current loss_val =  0.7394331392284134\n",
      "epochs =  3 , step =  4000 , current loss_val =  0.7961486631666836\n",
      "epochs =  3 , step =  5000 , current loss_val =  0.7412925227969223\n",
      "epochs =  3 , step =  6000 , current loss_val =  0.7796642350807356\n",
      "epochs =  3 , step =  7000 , current loss_val =  0.9367181060012043\n",
      "epochs =  3 , step =  8000 , current loss_val =  0.7230884388242839\n",
      "epochs =  3 , step =  9000 , current loss_val =  0.8376689188375901\n",
      "epochs =  3 , step =  10000 , current loss_val =  0.9789626467239089\n",
      "epochs =  3 , step =  11000 , current loss_val =  0.8038169015768414\n",
      "epochs =  3 , step =  12000 , current loss_val =  0.8107380519124286\n",
      "epochs =  3 , step =  13000 , current loss_val =  0.910869659033689\n",
      "epochs =  3 , step =  14000 , current loss_val =  0.8103680227468985\n",
      "epochs =  3 , step =  15000 , current loss_val =  0.8036012465048604\n",
      "epochs =  3 , step =  16000 , current loss_val =  0.8180588464135172\n",
      "epochs =  3 , step =  17000 , current loss_val =  1.2039656582817144\n",
      "epochs =  3 , step =  18000 , current loss_val =  0.7863497380031605\n",
      "epochs =  3 , step =  19000 , current loss_val =  0.8202628510434818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  3 , step =  20000 , current loss_val =  0.8757975163863698\n",
      "epochs =  3 , step =  21000 , current loss_val =  0.8711200339417784\n",
      "epochs =  3 , step =  22000 , current loss_val =  0.7795910022245015\n",
      "epochs =  3 , step =  23000 , current loss_val =  0.8702315416689913\n",
      "epochs =  3 , step =  24000 , current loss_val =  1.114584818328898\n",
      "epochs =  3 , step =  25000 , current loss_val =  0.7753966159390692\n",
      "epochs =  3 , step =  26000 , current loss_val =  0.8852748540328442\n",
      "epochs =  3 , step =  27000 , current loss_val =  0.7858470689517371\n",
      "epochs =  3 , step =  28000 , current loss_val =  0.8495169364803896\n",
      "epochs =  3 , step =  29000 , current loss_val =  0.8127272146114143\n",
      "epochs =  3 , step =  30000 , current loss_val =  0.7675291304339171\n",
      "epochs =  3 , step =  31000 , current loss_val =  0.7427453223009512\n",
      "epochs =  3 , step =  32000 , current loss_val =  0.7728579859341485\n",
      "epochs =  3 , step =  33000 , current loss_val =  0.8370975719546857\n",
      "epochs =  3 , step =  34000 , current loss_val =  0.8198650111216234\n",
      "epochs =  3 , step =  35000 , current loss_val =  0.8712136787851165\n",
      "epochs =  3 , step =  36000 , current loss_val =  0.7497364407347397\n",
      "epochs =  3 , step =  37000 , current loss_val =  0.7780588014020533\n",
      "epochs =  3 , step =  38000 , current loss_val =  0.8434081666713885\n",
      "epochs =  3 , step =  39000 , current loss_val =  0.7397720595945504\n",
      "epochs =  3 , step =  40000 , current loss_val =  0.841410763239806\n",
      "epochs =  3 , step =  41000 , current loss_val =  0.8250490806586147\n",
      "epochs =  3 , step =  42000 , current loss_val =  0.7036741632675881\n",
      "epochs =  3 , step =  43000 , current loss_val =  0.866179865030031\n",
      "epochs =  3 , step =  44000 , current loss_val =  0.7648148310780865\n",
      "epochs =  3 , step =  45000 , current loss_val =  0.9615540840463512\n",
      "epochs =  3 , step =  46000 , current loss_val =  0.8865448208831791\n",
      "epochs =  3 , step =  47000 , current loss_val =  0.8351862072733709\n",
      "\n",
      "current epochs =  3  , current training accuracy =  96.3  %\n",
      "current epochs =  3  , current validation accuracy =  95.5  %\n",
      "\n",
      "epochs =  4 , step =  0 , current loss_val =  0.8107312027921341\n",
      "epochs =  4 , step =  1000 , current loss_val =  0.7512879913977986\n",
      "epochs =  4 , step =  2000 , current loss_val =  3.6106237884907877\n",
      "epochs =  4 , step =  3000 , current loss_val =  0.7332808086362335\n",
      "epochs =  4 , step =  4000 , current loss_val =  0.7868115952013273\n",
      "epochs =  4 , step =  5000 , current loss_val =  0.7544990600292807\n",
      "epochs =  4 , step =  6000 , current loss_val =  0.8011963649199112\n",
      "epochs =  4 , step =  7000 , current loss_val =  0.8468899944811993\n",
      "epochs =  4 , step =  8000 , current loss_val =  0.7426642221889775\n",
      "epochs =  4 , step =  9000 , current loss_val =  0.853996378699069\n",
      "epochs =  4 , step =  10000 , current loss_val =  0.9420597562501725\n",
      "epochs =  4 , step =  11000 , current loss_val =  0.8171473495787063\n",
      "epochs =  4 , step =  12000 , current loss_val =  0.8176838809609911\n",
      "epochs =  4 , step =  13000 , current loss_val =  0.8587649830543249\n",
      "epochs =  4 , step =  14000 , current loss_val =  0.8241628031922118\n",
      "epochs =  4 , step =  15000 , current loss_val =  0.8237086257771447\n",
      "epochs =  4 , step =  16000 , current loss_val =  0.8566643203066632\n",
      "epochs =  4 , step =  17000 , current loss_val =  1.0802153738490596\n",
      "epochs =  4 , step =  18000 , current loss_val =  0.8028102013618698\n",
      "epochs =  4 , step =  19000 , current loss_val =  0.825689437945724\n",
      "epochs =  4 , step =  20000 , current loss_val =  0.8959673466100072\n",
      "epochs =  4 , step =  21000 , current loss_val =  0.8788486755973237\n",
      "epochs =  4 , step =  22000 , current loss_val =  0.812372526592369\n",
      "epochs =  4 , step =  23000 , current loss_val =  0.8738162599985944\n",
      "epochs =  4 , step =  24000 , current loss_val =  1.1062396292040373\n",
      "epochs =  4 , step =  25000 , current loss_val =  0.8127406027856733\n",
      "epochs =  4 , step =  26000 , current loss_val =  0.8958151219801667\n",
      "epochs =  4 , step =  27000 , current loss_val =  0.8020978258693943\n",
      "epochs =  4 , step =  28000 , current loss_val =  0.8488368246623547\n",
      "epochs =  4 , step =  29000 , current loss_val =  0.832758741275781\n",
      "epochs =  4 , step =  30000 , current loss_val =  0.7902720000221749\n",
      "epochs =  4 , step =  31000 , current loss_val =  0.7445417719795767\n",
      "epochs =  4 , step =  32000 , current loss_val =  0.7565301277797986\n",
      "epochs =  4 , step =  33000 , current loss_val =  0.8758648012887756\n",
      "epochs =  4 , step =  34000 , current loss_val =  0.7970833591792629\n",
      "epochs =  4 , step =  35000 , current loss_val =  0.8667851651344957\n",
      "epochs =  4 , step =  36000 , current loss_val =  0.7591703138236604\n",
      "epochs =  4 , step =  37000 , current loss_val =  0.7744321072164416\n",
      "epochs =  4 , step =  38000 , current loss_val =  0.8660030847892163\n",
      "epochs =  4 , step =  39000 , current loss_val =  0.7445000831097325\n",
      "epochs =  4 , step =  40000 , current loss_val =  0.8649753244358283\n",
      "epochs =  4 , step =  41000 , current loss_val =  0.8227866138506568\n",
      "epochs =  4 , step =  42000 , current loss_val =  0.7090885215673173\n",
      "epochs =  4 , step =  43000 , current loss_val =  0.8891910786873308\n",
      "epochs =  4 , step =  44000 , current loss_val =  0.775771830354071\n",
      "epochs =  4 , step =  45000 , current loss_val =  0.9444512100760776\n",
      "epochs =  4 , step =  46000 , current loss_val =  0.8989919074081992\n",
      "epochs =  4 , step =  47000 , current loss_val =  0.8265520033621211\n",
      "\n",
      "current epochs =  4  , current training accuracy =  97.0  %\n",
      "current epochs =  4  , current validation accuracy =  96.0  %\n",
      "\n",
      "epochs =  5 , step =  0 , current loss_val =  0.8222979349728479\n",
      "epochs =  5 , step =  1000 , current loss_val =  0.768706133322697\n",
      "epochs =  5 , step =  2000 , current loss_val =  3.9074439628955893\n",
      "epochs =  5 , step =  3000 , current loss_val =  0.7506203755672515\n",
      "epochs =  5 , step =  4000 , current loss_val =  0.7898981727370598\n",
      "epochs =  5 , step =  5000 , current loss_val =  0.7750391746176717\n",
      "epochs =  5 , step =  6000 , current loss_val =  0.8191144509528847\n",
      "epochs =  5 , step =  7000 , current loss_val =  0.8294637525256662\n",
      "epochs =  5 , step =  8000 , current loss_val =  0.7606667827563126\n",
      "epochs =  5 , step =  9000 , current loss_val =  0.8722235652203169\n",
      "epochs =  5 , step =  10000 , current loss_val =  0.9004539410165507\n",
      "epochs =  5 , step =  11000 , current loss_val =  0.8304583865625027\n",
      "epochs =  5 , step =  12000 , current loss_val =  0.8376013543016415\n",
      "epochs =  5 , step =  13000 , current loss_val =  0.8003230459058459\n",
      "epochs =  5 , step =  14000 , current loss_val =  0.8409932894265144\n",
      "epochs =  5 , step =  15000 , current loss_val =  0.8441006948793301\n",
      "epochs =  5 , step =  16000 , current loss_val =  0.8561111585054615\n",
      "epochs =  5 , step =  17000 , current loss_val =  1.0754851570689385\n",
      "epochs =  5 , step =  18000 , current loss_val =  0.8115873879380194\n",
      "epochs =  5 , step =  19000 , current loss_val =  0.8140245960517726\n",
      "epochs =  5 , step =  20000 , current loss_val =  0.9097423199635117\n",
      "epochs =  5 , step =  21000 , current loss_val =  0.8884052347119518\n",
      "epochs =  5 , step =  22000 , current loss_val =  0.8494963764469985\n",
      "epochs =  5 , step =  23000 , current loss_val =  0.8758599254298467\n",
      "epochs =  5 , step =  24000 , current loss_val =  1.1054188791635084\n",
      "epochs =  5 , step =  25000 , current loss_val =  0.8403631334477003\n",
      "epochs =  5 , step =  26000 , current loss_val =  0.9060266785897036\n",
      "epochs =  5 , step =  27000 , current loss_val =  0.822649708560069\n",
      "epochs =  5 , step =  28000 , current loss_val =  0.8525537854685865\n",
      "epochs =  5 , step =  29000 , current loss_val =  0.8462432263642994\n",
      "epochs =  5 , step =  30000 , current loss_val =  0.8053139693090903\n",
      "epochs =  5 , step =  31000 , current loss_val =  0.7511183731768798\n",
      "epochs =  5 , step =  32000 , current loss_val =  0.7555593636068908\n",
      "epochs =  5 , step =  33000 , current loss_val =  0.8973324210551161\n",
      "epochs =  5 , step =  34000 , current loss_val =  0.7957147366858808\n",
      "epochs =  5 , step =  35000 , current loss_val =  0.8671698116918738\n",
      "epochs =  5 , step =  36000 , current loss_val =  0.7714581789629689\n",
      "epochs =  5 , step =  37000 , current loss_val =  0.7628868933880446\n",
      "epochs =  5 , step =  38000 , current loss_val =  0.8891957613701077\n",
      "epochs =  5 , step =  39000 , current loss_val =  0.7493180918879253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  5 , step =  40000 , current loss_val =  0.8931264117899189\n",
      "epochs =  5 , step =  41000 , current loss_val =  0.8209207417466967\n",
      "epochs =  5 , step =  42000 , current loss_val =  0.7164803310211109\n",
      "epochs =  5 , step =  43000 , current loss_val =  0.9029197531873359\n",
      "epochs =  5 , step =  44000 , current loss_val =  0.7777369906056603\n",
      "epochs =  5 , step =  45000 , current loss_val =  0.9418726898031423\n",
      "epochs =  5 , step =  46000 , current loss_val =  0.8988794739080505\n",
      "epochs =  5 , step =  47000 , current loss_val =  0.8268558774351898\n",
      "\n",
      "current epochs =  5  , current training accuracy =  97.39999999999999  %\n",
      "current epochs =  5  , current validation accuracy =  96.39999999999999  %\n",
      "\n",
      "epochs =  6 , step =  0 , current loss_val =  0.8308879082749601\n",
      "epochs =  6 , step =  1000 , current loss_val =  0.7831503565309137\n",
      "epochs =  6 , step =  2000 , current loss_val =  4.233041592753958\n",
      "epochs =  6 , step =  3000 , current loss_val =  0.7760172813797759\n",
      "epochs =  6 , step =  4000 , current loss_val =  0.7991402081186296\n",
      "epochs =  6 , step =  5000 , current loss_val =  0.7958318101863904\n",
      "epochs =  6 , step =  6000 , current loss_val =  0.8332012424882753\n",
      "epochs =  6 , step =  7000 , current loss_val =  0.8174153374827224\n",
      "epochs =  6 , step =  8000 , current loss_val =  0.7763186633924818\n",
      "epochs =  6 , step =  9000 , current loss_val =  0.8961438855032682\n",
      "epochs =  6 , step =  10000 , current loss_val =  0.8911149653362984\n",
      "epochs =  6 , step =  11000 , current loss_val =  0.8434394237724785\n",
      "epochs =  6 , step =  12000 , current loss_val =  0.8616913019796251\n",
      "epochs =  6 , step =  13000 , current loss_val =  0.7747045330908882\n",
      "epochs =  6 , step =  14000 , current loss_val =  0.8605643169953077\n",
      "epochs =  6 , step =  15000 , current loss_val =  0.8567147854217968\n",
      "epochs =  6 , step =  16000 , current loss_val =  0.8592371349138237\n",
      "epochs =  6 , step =  17000 , current loss_val =  1.0917048546534676\n",
      "epochs =  6 , step =  18000 , current loss_val =  0.8265416470234775\n",
      "epochs =  6 , step =  19000 , current loss_val =  0.8117037550809553\n",
      "epochs =  6 , step =  20000 , current loss_val =  0.9190518441116581\n",
      "epochs =  6 , step =  21000 , current loss_val =  0.8994568783319129\n",
      "epochs =  6 , step =  22000 , current loss_val =  0.86778367482228\n",
      "epochs =  6 , step =  23000 , current loss_val =  0.8658738766989276\n",
      "epochs =  6 , step =  24000 , current loss_val =  1.0987631775779345\n",
      "epochs =  6 , step =  25000 , current loss_val =  0.8672829505524879\n",
      "epochs =  6 , step =  26000 , current loss_val =  0.9189658252537405\n",
      "epochs =  6 , step =  27000 , current loss_val =  0.8414616707346312\n",
      "epochs =  6 , step =  28000 , current loss_val =  0.8588254783911097\n",
      "epochs =  6 , step =  29000 , current loss_val =  0.85422638392683\n",
      "epochs =  6 , step =  30000 , current loss_val =  0.8164899107615511\n",
      "epochs =  6 , step =  31000 , current loss_val =  0.7710839916383174\n",
      "epochs =  6 , step =  32000 , current loss_val =  0.7652892162583784\n",
      "epochs =  6 , step =  33000 , current loss_val =  0.9118263717420371\n",
      "epochs =  6 , step =  34000 , current loss_val =  0.8075850166517099\n",
      "epochs =  6 , step =  35000 , current loss_val =  0.8722021609347045\n",
      "epochs =  6 , step =  36000 , current loss_val =  0.7812678798037727\n",
      "epochs =  6 , step =  37000 , current loss_val =  0.7604894274422236\n",
      "epochs =  6 , step =  38000 , current loss_val =  0.9154421230808327\n",
      "epochs =  6 , step =  39000 , current loss_val =  0.7568477392304129\n",
      "epochs =  6 , step =  40000 , current loss_val =  0.9207398525615188\n",
      "epochs =  6 , step =  41000 , current loss_val =  0.8309880153090287\n",
      "epochs =  6 , step =  42000 , current loss_val =  0.7281228702275601\n",
      "epochs =  6 , step =  43000 , current loss_val =  0.9148206428503121\n",
      "epochs =  6 , step =  44000 , current loss_val =  0.7851288795736204\n",
      "epochs =  6 , step =  45000 , current loss_val =  0.9421966616053187\n",
      "epochs =  6 , step =  46000 , current loss_val =  0.8879392786277218\n",
      "epochs =  6 , step =  47000 , current loss_val =  0.821787169032948\n",
      "\n",
      "current epochs =  6  , current training accuracy =  97.8  %\n",
      "current epochs =  6  , current validation accuracy =  96.7  %\n",
      "\n",
      "epochs =  7 , step =  0 , current loss_val =  0.8477074987738484\n",
      "epochs =  7 , step =  1000 , current loss_val =  0.7978762540498323\n",
      "epochs =  7 , step =  2000 , current loss_val =  4.490014207546056\n",
      "epochs =  7 , step =  3000 , current loss_val =  0.7995737480932531\n",
      "epochs =  7 , step =  4000 , current loss_val =  0.8080744505725848\n",
      "epochs =  7 , step =  5000 , current loss_val =  0.8176033654228485\n",
      "epochs =  7 , step =  6000 , current loss_val =  0.846671006543586\n",
      "epochs =  7 , step =  7000 , current loss_val =  0.8127033000419632\n",
      "epochs =  7 , step =  8000 , current loss_val =  0.7972049797231466\n",
      "epochs =  7 , step =  9000 , current loss_val =  0.926941380829282\n",
      "epochs =  7 , step =  10000 , current loss_val =  0.892501847802583\n",
      "epochs =  7 , step =  11000 , current loss_val =  0.8633048507727089\n",
      "epochs =  7 , step =  12000 , current loss_val =  0.8859218560865687\n",
      "epochs =  7 , step =  13000 , current loss_val =  0.7690023682937768\n",
      "epochs =  7 , step =  14000 , current loss_val =  0.8835607242080249\n",
      "epochs =  7 , step =  15000 , current loss_val =  0.8717381166528761\n",
      "epochs =  7 , step =  16000 , current loss_val =  0.8668092523187033\n",
      "epochs =  7 , step =  17000 , current loss_val =  1.1130957108602426\n",
      "epochs =  7 , step =  18000 , current loss_val =  0.8427686982010494\n",
      "epochs =  7 , step =  19000 , current loss_val =  0.8201090060552567\n",
      "epochs =  7 , step =  20000 , current loss_val =  0.9236003609343499\n",
      "epochs =  7 , step =  21000 , current loss_val =  0.9069284017138896\n",
      "epochs =  7 , step =  22000 , current loss_val =  0.8770621014994756\n",
      "epochs =  7 , step =  23000 , current loss_val =  0.8368367424219869\n",
      "epochs =  7 , step =  24000 , current loss_val =  1.0827032306519497\n",
      "epochs =  7 , step =  25000 , current loss_val =  0.8898226776499096\n",
      "epochs =  7 , step =  26000 , current loss_val =  0.9349523692438253\n",
      "epochs =  7 , step =  27000 , current loss_val =  0.8585867947994135\n",
      "epochs =  7 , step =  28000 , current loss_val =  0.8712135718015777\n",
      "epochs =  7 , step =  29000 , current loss_val =  0.8587646495159357\n",
      "epochs =  7 , step =  30000 , current loss_val =  0.827183719195413\n",
      "epochs =  7 , step =  31000 , current loss_val =  0.7924822748012281\n",
      "epochs =  7 , step =  32000 , current loss_val =  0.7767591011487559\n",
      "epochs =  7 , step =  33000 , current loss_val =  0.9304707523559804\n",
      "epochs =  7 , step =  34000 , current loss_val =  0.8230735011613797\n",
      "epochs =  7 , step =  35000 , current loss_val =  0.8747582280989831\n",
      "epochs =  7 , step =  36000 , current loss_val =  0.7927618184776588\n",
      "epochs =  7 , step =  37000 , current loss_val =  0.7672167528344309\n",
      "epochs =  7 , step =  38000 , current loss_val =  0.9373218951804975\n",
      "epochs =  7 , step =  39000 , current loss_val =  0.7643490134427854\n",
      "epochs =  7 , step =  40000 , current loss_val =  0.942221455755911\n",
      "epochs =  7 , step =  41000 , current loss_val =  0.8508089006420144\n",
      "epochs =  7 , step =  42000 , current loss_val =  0.7426564926681952\n",
      "epochs =  7 , step =  43000 , current loss_val =  0.9251710577829981\n",
      "epochs =  7 , step =  44000 , current loss_val =  0.7944223555652626\n",
      "epochs =  7 , step =  45000 , current loss_val =  0.9355409289087342\n",
      "epochs =  7 , step =  46000 , current loss_val =  0.89811367183685\n",
      "epochs =  7 , step =  47000 , current loss_val =  0.8241861222613843\n",
      "\n",
      "current epochs =  7  , current training accuracy =  98.0  %\n",
      "current epochs =  7  , current validation accuracy =  96.8  %\n",
      "\n",
      "epochs =  8 , step =  0 , current loss_val =  0.8660146030581425\n",
      "epochs =  8 , step =  1000 , current loss_val =  0.8101639459329839\n",
      "epochs =  8 , step =  2000 , current loss_val =  4.471542307502236\n",
      "epochs =  8 , step =  3000 , current loss_val =  0.8307649562767139\n",
      "epochs =  8 , step =  4000 , current loss_val =  0.8182604956097972\n",
      "epochs =  8 , step =  5000 , current loss_val =  0.8377634228558997\n",
      "epochs =  8 , step =  6000 , current loss_val =  0.860384138921136\n",
      "epochs =  8 , step =  7000 , current loss_val =  0.8194656493275498\n",
      "epochs =  8 , step =  8000 , current loss_val =  0.8167536508776461\n",
      "epochs =  8 , step =  9000 , current loss_val =  0.9613523993762723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  8 , step =  10000 , current loss_val =  0.8965529926422643\n",
      "epochs =  8 , step =  11000 , current loss_val =  0.8810053379038929\n",
      "epochs =  8 , step =  12000 , current loss_val =  0.902076772184153\n",
      "epochs =  8 , step =  13000 , current loss_val =  0.7762922022777299\n",
      "epochs =  8 , step =  14000 , current loss_val =  0.9068131861054306\n",
      "epochs =  8 , step =  15000 , current loss_val =  0.8895679686207558\n",
      "epochs =  8 , step =  16000 , current loss_val =  0.8675508653937929\n",
      "epochs =  8 , step =  17000 , current loss_val =  1.1398262995458992\n",
      "epochs =  8 , step =  18000 , current loss_val =  0.8602850465087557\n",
      "epochs =  8 , step =  19000 , current loss_val =  0.8361487744749225\n",
      "epochs =  8 , step =  20000 , current loss_val =  0.9272810670857576\n",
      "epochs =  8 , step =  21000 , current loss_val =  0.912007097069764\n",
      "epochs =  8 , step =  22000 , current loss_val =  0.8911716046166771\n",
      "epochs =  8 , step =  23000 , current loss_val =  0.8306582232563975\n",
      "epochs =  8 , step =  24000 , current loss_val =  1.0769137559692987\n",
      "epochs =  8 , step =  25000 , current loss_val =  0.9045146575142824\n",
      "epochs =  8 , step =  26000 , current loss_val =  0.9522266711572489\n",
      "epochs =  8 , step =  27000 , current loss_val =  0.8776799819507635\n",
      "epochs =  8 , step =  28000 , current loss_val =  0.8868635565913905\n",
      "epochs =  8 , step =  29000 , current loss_val =  0.8600506774455726\n",
      "epochs =  8 , step =  30000 , current loss_val =  0.8447254036626795\n",
      "epochs =  8 , step =  31000 , current loss_val =  0.8137741073981678\n",
      "epochs =  8 , step =  32000 , current loss_val =  0.7881741512658609\n",
      "epochs =  8 , step =  33000 , current loss_val =  0.9518656925433562\n",
      "epochs =  8 , step =  34000 , current loss_val =  0.8373691757407817\n",
      "epochs =  8 , step =  35000 , current loss_val =  0.8795580799714487\n",
      "epochs =  8 , step =  36000 , current loss_val =  0.807100253542532\n",
      "epochs =  8 , step =  37000 , current loss_val =  0.775648724101491\n",
      "epochs =  8 , step =  38000 , current loss_val =  0.9538959403148931\n",
      "epochs =  8 , step =  39000 , current loss_val =  0.7749178342539733\n",
      "epochs =  8 , step =  40000 , current loss_val =  0.9602621650713293\n",
      "epochs =  8 , step =  41000 , current loss_val =  0.8720563151565357\n",
      "epochs =  8 , step =  42000 , current loss_val =  0.7591205665694262\n",
      "epochs =  8 , step =  43000 , current loss_val =  0.9344486442076633\n",
      "epochs =  8 , step =  44000 , current loss_val =  0.8033376963914829\n",
      "epochs =  8 , step =  45000 , current loss_val =  0.9332467811195514\n",
      "epochs =  8 , step =  46000 , current loss_val =  0.9145640291527166\n",
      "epochs =  8 , step =  47000 , current loss_val =  0.8244451002261113\n",
      "\n",
      "current epochs =  8  , current training accuracy =  98.2  %\n",
      "current epochs =  8  , current validation accuracy =  96.89999999999999  %\n",
      "\n",
      "epochs =  9 , step =  0 , current loss_val =  0.8843553749808236\n",
      "epochs =  9 , step =  1000 , current loss_val =  0.825151843596289\n",
      "epochs =  9 , step =  2000 , current loss_val =  4.437049310282313\n",
      "epochs =  9 , step =  3000 , current loss_val =  0.8741542747750862\n",
      "epochs =  9 , step =  4000 , current loss_val =  0.8296750534976366\n",
      "epochs =  9 , step =  5000 , current loss_val =  0.8516578501922931\n",
      "epochs =  9 , step =  6000 , current loss_val =  0.8735697639901022\n",
      "epochs =  9 , step =  7000 , current loss_val =  0.8268622730474053\n",
      "epochs =  9 , step =  8000 , current loss_val =  0.8334718209777817\n",
      "epochs =  9 , step =  9000 , current loss_val =  0.9922404215776622\n",
      "epochs =  9 , step =  10000 , current loss_val =  0.9049427463192948\n",
      "epochs =  9 , step =  11000 , current loss_val =  0.8926080497661374\n",
      "epochs =  9 , step =  12000 , current loss_val =  0.9153151644080014\n",
      "epochs =  9 , step =  13000 , current loss_val =  0.7908207959922134\n",
      "epochs =  9 , step =  14000 , current loss_val =  0.9274693076437585\n",
      "epochs =  9 , step =  15000 , current loss_val =  0.9050989155986509\n",
      "epochs =  9 , step =  16000 , current loss_val =  0.8730930653197772\n",
      "epochs =  9 , step =  17000 , current loss_val =  1.1626613193254427\n",
      "epochs =  9 , step =  18000 , current loss_val =  0.8764919260513627\n",
      "epochs =  9 , step =  19000 , current loss_val =  0.8522458820217097\n",
      "epochs =  9 , step =  20000 , current loss_val =  0.9323899809283993\n",
      "epochs =  9 , step =  21000 , current loss_val =  0.9193826377016098\n",
      "epochs =  9 , step =  22000 , current loss_val =  0.9027090902350904\n",
      "epochs =  9 , step =  23000 , current loss_val =  0.8350294985214092\n",
      "epochs =  9 , step =  24000 , current loss_val =  1.0853534779401217\n",
      "epochs =  9 , step =  25000 , current loss_val =  0.9150533337308453\n",
      "epochs =  9 , step =  26000 , current loss_val =  0.9715226912123865\n",
      "epochs =  9 , step =  27000 , current loss_val =  0.89698893044885\n",
      "epochs =  9 , step =  28000 , current loss_val =  0.9043157700139512\n",
      "epochs =  9 , step =  29000 , current loss_val =  0.8630187688868102\n",
      "epochs =  9 , step =  30000 , current loss_val =  0.8634590585936054\n",
      "epochs =  9 , step =  31000 , current loss_val =  0.8356199564190792\n",
      "epochs =  9 , step =  32000 , current loss_val =  0.799515493782344\n",
      "epochs =  9 , step =  33000 , current loss_val =  0.9751835539552197\n",
      "epochs =  9 , step =  34000 , current loss_val =  0.8508744527332887\n",
      "epochs =  9 , step =  35000 , current loss_val =  0.8881543074440299\n",
      "epochs =  9 , step =  36000 , current loss_val =  0.8219692973176541\n",
      "epochs =  9 , step =  37000 , current loss_val =  0.7809653570382648\n",
      "epochs =  9 , step =  38000 , current loss_val =  0.973147505749795\n",
      "epochs =  9 , step =  39000 , current loss_val =  0.7920195270211812\n",
      "epochs =  9 , step =  40000 , current loss_val =  0.9763177910950743\n",
      "epochs =  9 , step =  41000 , current loss_val =  0.8925612116050858\n",
      "epochs =  9 , step =  42000 , current loss_val =  0.7765482157977907\n",
      "epochs =  9 , step =  43000 , current loss_val =  0.94251360435285\n",
      "epochs =  9 , step =  44000 , current loss_val =  0.8110450058763247\n",
      "epochs =  9 , step =  45000 , current loss_val =  0.9349813412755729\n",
      "epochs =  9 , step =  46000 , current loss_val =  0.9324478965969809\n",
      "epochs =  9 , step =  47000 , current loss_val =  0.8243095901963897\n",
      "\n",
      "current epochs =  9  , current training accuracy =  98.3  %\n",
      "current epochs =  9  , current validation accuracy =  96.89999999999999  %\n",
      "\n",
      "epochs =  10 , step =  0 , current loss_val =  0.8978579518248102\n",
      "epochs =  10 , step =  1000 , current loss_val =  0.8436126444938762\n",
      "epochs =  10 , step =  2000 , current loss_val =  4.413434077389317\n",
      "epochs =  10 , step =  3000 , current loss_val =  0.9227498732420747\n",
      "epochs =  10 , step =  4000 , current loss_val =  0.8435791460150293\n",
      "epochs =  10 , step =  5000 , current loss_val =  0.8614524200324847\n",
      "epochs =  10 , step =  6000 , current loss_val =  0.8876713523226921\n",
      "epochs =  10 , step =  7000 , current loss_val =  0.8335702788436611\n",
      "epochs =  10 , step =  8000 , current loss_val =  0.8485959646550942\n",
      "epochs =  10 , step =  9000 , current loss_val =  1.019656221328258\n",
      "epochs =  10 , step =  10000 , current loss_val =  0.9142468555330633\n",
      "epochs =  10 , step =  11000 , current loss_val =  0.8983706357214987\n",
      "epochs =  10 , step =  12000 , current loss_val =  0.9320167885177753\n",
      "epochs =  10 , step =  13000 , current loss_val =  0.8135842153305699\n",
      "epochs =  10 , step =  14000 , current loss_val =  0.9453423675295524\n",
      "epochs =  10 , step =  15000 , current loss_val =  0.9173081033441155\n",
      "epochs =  10 , step =  16000 , current loss_val =  0.88503627968132\n",
      "epochs =  10 , step =  17000 , current loss_val =  1.1730414276293568\n",
      "epochs =  10 , step =  18000 , current loss_val =  0.8917521869665961\n",
      "epochs =  10 , step =  19000 , current loss_val =  0.8640068002496355\n",
      "epochs =  10 , step =  20000 , current loss_val =  0.9386608785993602\n",
      "epochs =  10 , step =  21000 , current loss_val =  0.929392256418393\n",
      "epochs =  10 , step =  22000 , current loss_val =  0.9158374963613439\n",
      "epochs =  10 , step =  23000 , current loss_val =  0.8431794508223537\n",
      "epochs =  10 , step =  24000 , current loss_val =  1.0925180291338683\n",
      "epochs =  10 , step =  25000 , current loss_val =  0.9263958054210482\n",
      "epochs =  10 , step =  26000 , current loss_val =  0.9875895663195569\n",
      "epochs =  10 , step =  27000 , current loss_val =  0.9147754136284338\n",
      "epochs =  10 , step =  28000 , current loss_val =  0.9244128382775864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  10 , step =  29000 , current loss_val =  0.8681185849922433\n",
      "epochs =  10 , step =  30000 , current loss_val =  0.8789931824851196\n",
      "epochs =  10 , step =  31000 , current loss_val =  0.8553619765142764\n",
      "epochs =  10 , step =  32000 , current loss_val =  0.8097795205889288\n",
      "epochs =  10 , step =  33000 , current loss_val =  0.9985636180611336\n",
      "epochs =  10 , step =  34000 , current loss_val =  0.8616798191832276\n",
      "epochs =  10 , step =  35000 , current loss_val =  0.8966387089883716\n",
      "epochs =  10 , step =  36000 , current loss_val =  0.8346421469246169\n",
      "epochs =  10 , step =  37000 , current loss_val =  0.7855167509794566\n",
      "epochs =  10 , step =  38000 , current loss_val =  0.9962395360650426\n",
      "epochs =  10 , step =  39000 , current loss_val =  0.8067831341077054\n",
      "epochs =  10 , step =  40000 , current loss_val =  0.9938657733492269\n",
      "epochs =  10 , step =  41000 , current loss_val =  0.9118251930535552\n",
      "epochs =  10 , step =  42000 , current loss_val =  0.7921544329335236\n",
      "epochs =  10 , step =  43000 , current loss_val =  0.9479695882420428\n",
      "epochs =  10 , step =  44000 , current loss_val =  0.8192615347761556\n",
      "epochs =  10 , step =  45000 , current loss_val =  0.9407076705640682\n",
      "epochs =  10 , step =  46000 , current loss_val =  0.9502026366151969\n",
      "epochs =  10 , step =  47000 , current loss_val =  0.8241127656849759\n",
      "\n",
      "current epochs =  10  , current training accuracy =  98.4  %\n",
      "current epochs =  10  , current validation accuracy =  97.0  %\n",
      "\n",
      "\n",
      "elapsed time =  0:08:29.919092\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter\n",
    "i_nodes = training_data.shape[1] - 1    # input nodes 개수\n",
    "h1_nodes = 100     # hidden 1 nodes\n",
    "o_nodes = 10       # output nodes\n",
    "lr = 0.1           # learning rate\n",
    "epochs = 10         # epochs\n",
    "\n",
    "# 손실함수 값을 저장할 list 생성\n",
    "loss_val_list = []\n",
    "\n",
    "# 정확도 저장 리스트\n",
    "training_accuracy_list = []\n",
    "validation_accuracy_list = []\n",
    "\n",
    "# 객체 생성\n",
    "nn = NeuralNetwork(i_nodes, h1_nodes, o_nodes, lr)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for step in range(len(training_data)):  # train\n",
    "    \n",
    "        # input_data, target_data normalize        \n",
    "        target_data = np.zeros(o_nodes) + 0.01    \n",
    "        target_data[int(training_data[step, 0])] = 0.99\n",
    "    \n",
    "        input_data = ((training_data[step, 1:] / 255.0) * 0.99) + 0.01\n",
    "    \n",
    "        nn.train( np.array(input_data, ndmin=2), np.array(target_data, ndmin=2) )\n",
    "    \n",
    "        if step % 1000 == 0:\n",
    "            print(\"epochs = \", i+1, \", step = \", step,  \", current loss_val = \", nn.loss_val())\n",
    "            \n",
    "        # 손실함수 값 저장 per step\n",
    "        loss_val_list.append(nn.loss_val())    \n",
    "        \n",
    "    # 정확도 계산 및 저장 per epochs\n",
    "    (training_accuracy, index_label_prediction_list) = nn.accuracy(training_data[:, 1:], training_data[:, 0])\n",
    "    (validation_accuracy, index_label_prediction_list) = nn.accuracy(validation_data[:, 1:], validation_data[:, 0])\n",
    "    \n",
    "    print('\\ncurrent epochs = ', i+1,' , current training accuracy = ', 100*np.round(training_accuracy,3), ' %')\n",
    "    print('current epochs = ', i+1,' , current validation accuracy = ', 100*np.round(validation_accuracy,3), ' %\\n')\n",
    "        \n",
    "    training_accuracy_list.append(training_accuracy)\n",
    "    validation_accuracy_list.append(validation_accuracy)\n",
    "        \n",
    "        \n",
    "end_time = datetime.now() \n",
    "print(\"\\nelapsed time = \", end_time - start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Training / Validation Accuracy Trend')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.plot(training_accuracy_list, label='training acc')\n",
    "plt.plot(validation_accuracy_list, label='validation acc')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data.shape =  (10000, 785)\n",
      "test_data[0,0] =  7.0 , len(test_data[0]) =  785\n",
      "Accuracy =  97.15  %\n"
     ]
    }
   ],
   "source": [
    "# 0~9 숫자 이미지가 784개의 숫자 (28X28) 로 구성되어 있는 test data 읽어옴\n",
    "\n",
    "try:\n",
    "    \n",
    "    test_data = np.loadtxt('./mnist_test.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "    test_input_data = test_data[ : , 1: ]\n",
    "    test_target_data = test_data[ : , 0 ]\n",
    "\n",
    "    print(\"test_data.shape = \", test_data.shape)\n",
    "    print(\"test_data[0,0] = \", test_data[0,0], \", len(test_data[0]) = \", len(test_data[0]))\n",
    "\n",
    "    # measure accuracy\n",
    "    (accuracy_ret, index_label_prediction_list) = nn.accuracy(test_input_data, test_target_data)   \n",
    "\n",
    "    print('Accuracy = ', np.round(100*accuracy_ret, 3), ' %')\n",
    "    \n",
    "except Exception as err:\n",
    "    \n",
    "    print('Exception occur !!')\n",
    "    print(str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9715\n",
      "false prediction data num =  285\n"
     ]
    }
   ],
   "source": [
    "# 총 오답 개수\n",
    "total_test_data_num = len(test_data)\n",
    "false_prediction_data_num = len(index_label_prediction_list)\n",
    "\n",
    "print('accuracy = ', (total_test_data_num - false_prediction_data_num) / total_test_data_num)\n",
    "print('false prediction data num = ', false_prediction_data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[61, 8, 2], [149, 2, 4], [247, 4, 2], [259, 6, 0], [320, 9, 8], [321, 2, 7], [381, 3, 7], [445, 6, 0], [449, 3, 5], [495, 8, 0], [571, 4, 9], [578, 3, 7], [619, 1, 8], [659, 2, 8], [684, 7, 2], [691, 8, 4], [707, 4, 9], [720, 5, 8], [740, 4, 9], [874, 9, 4], [882, 9, 7], [938, 3, 5], [956, 1, 3], [965, 6, 0], [1003, 5, 3], [1014, 6, 5], [1039, 7, 9], [1044, 6, 8], [1107, 9, 8], [1112, 4, 6], [1114, 3, 8], [1156, 7, 8], [1166, 3, 5], [1182, 6, 8], [1204, 3, 2], [1208, 3, 8], [1226, 7, 2], [1232, 9, 4], [1242, 4, 9], [1247, 9, 5], [1260, 7, 1], [1283, 7, 2], [1289, 5, 9], [1299, 5, 7], [1319, 8, 3], [1325, 8, 6], [1326, 7, 1], [1328, 7, 8], [1348, 2, 6], [1378, 5, 6], [1393, 5, 3], [1414, 9, 7], [1494, 7, 0], [1500, 7, 1], [1522, 7, 9], [1530, 8, 7], [1549, 4, 6], [1553, 9, 3], [1554, 9, 8], [1609, 2, 6], [1621, 0, 6], [1626, 6, 5], [1678, 2, 0], [1681, 3, 7], [1709, 9, 3], [1717, 8, 0], [1751, 4, 2], [1754, 7, 2], [1790, 2, 7], [1868, 1, 2], [1901, 9, 4], [1938, 4, 6], [1941, 7, 2], [1952, 9, 5], [1982, 6, 5], [2016, 7, 2], [2024, 7, 9], [2043, 4, 8], [2053, 4, 9], [2098, 2, 0], [2109, 3, 7], [2118, 6, 0], [2129, 9, 2], [2130, 4, 9], [2135, 6, 1], [2148, 4, 9], [2182, 1, 2], [2185, 0, 5], [2186, 2, 0], [2224, 5, 6], [2266, 1, 6], [2272, 8, 0], [2293, 9, 6], [2325, 7, 2], [2358, 1, 3], [2369, 5, 9], [2406, 9, 8], [2408, 3, 8], [2422, 6, 4], [2433, 2, 1], [2462, 2, 0], [2488, 2, 4], [2534, 3, 5], [2560, 3, 5], [2589, 9, 8], [2598, 8, 2], [2607, 7, 2], [2648, 9, 0], [2654, 6, 1], [2730, 7, 4], [2771, 4, 9], [2877, 4, 7], [2896, 8, 0], [2907, 4, 9], [2921, 3, 2], [2927, 3, 2], [2939, 9, 7], [2952, 3, 5], [2953, 3, 8], [3005, 9, 1], [3030, 6, 8], [3060, 9, 7], [3073, 1, 2], [3117, 5, 9], [3189, 7, 6], [3240, 9, 8], [3284, 8, 7], [3330, 2, 8], [3376, 7, 9], [3422, 6, 0], [3490, 4, 9], [3503, 9, 1], [3520, 6, 4], [3533, 4, 9], [3558, 5, 0], [3567, 8, 5], [3597, 9, 3], [3681, 2, 8], [3718, 4, 9], [3751, 7, 2], [3757, 8, 3], [3767, 7, 2], [3780, 4, 6], [3806, 5, 8], [3808, 7, 3], [3811, 2, 3], [3818, 0, 6], [3853, 6, 5], [3876, 2, 8], [3893, 5, 6], [3906, 1, 3], [3926, 9, 3], [3941, 4, 2], [3943, 3, 5], [3946, 2, 8], [3976, 7, 1], [4063, 6, 5], [4065, 0, 2], [4075, 8, 3], [4078, 9, 3], [4140, 8, 2], [4163, 9, 0], [4176, 2, 6], [4201, 1, 7], [4224, 9, 7], [4248, 2, 8], [4289, 2, 7], [4369, 9, 4], [4425, 9, 4], [4433, 7, 3], [4435, 3, 7], [4437, 3, 2], [4443, 3, 2], [4477, 0, 6], [4497, 8, 7], [4536, 6, 5], [4575, 4, 2], [4578, 7, 9], [4601, 8, 4], [4615, 2, 4], [4635, 3, 5], [4690, 7, 2], [4751, 4, 6], [4761, 9, 8], [4807, 8, 3], [4814, 6, 0], [4823, 9, 4], [4837, 7, 2], [4874, 9, 0], [4876, 2, 4], [4879, 8, 6], [4880, 0, 8], [4886, 7, 8], [4950, 2, 3], [4956, 8, 4], [4966, 7, 9], [4990, 3, 2], [5140, 3, 6], [5331, 1, 6], [5457, 1, 8], [5600, 7, 9], [5623, 3, 5], [5634, 2, 3], [5642, 1, 5], [5734, 3, 7], [5749, 8, 5], [5842, 4, 7], [5887, 7, 0], [5888, 4, 0], [5936, 4, 9], [5955, 3, 8], [5973, 3, 8], [6023, 3, 8], [6035, 2, 0], [6042, 5, 8], [6045, 3, 9], [6046, 3, 8], [6059, 3, 9], [6065, 3, 8], [6091, 9, 0], [6166, 9, 3], [6172, 9, 0], [6400, 0, 6], [6505, 9, 0], [6555, 8, 9], [6558, 6, 2], [6560, 9, 8], [6571, 9, 7], [6572, 1, 5], [6574, 2, 6], [6577, 7, 2], [6597, 0, 7], [6608, 9, 5], [6625, 8, 7], [6632, 9, 5], [6651, 0, 5], [6783, 1, 6], [7216, 0, 3], [7432, 7, 1], [7434, 4, 8], [7451, 5, 6], [7849, 3, 4], [7856, 1, 8], [7886, 2, 4], [8020, 1, 8], [8094, 2, 8], [8183, 8, 5], [8246, 3, 8], [8277, 3, 5], [8308, 3, 5], [8362, 3, 5], [8406, 4, 9], [8519, 7, 3], [8520, 4, 9], [8522, 8, 6], [8527, 4, 9], [9009, 7, 2], [9015, 7, 2], [9016, 0, 5], [9019, 7, 2], [9024, 7, 2], [9071, 1, 8], [9280, 8, 5], [9530, 9, 8], [9587, 9, 4], [9634, 0, 3], [9642, 9, 7], [9669, 4, 5], [9679, 6, 5], [9692, 9, 7], [9716, 2, 5], [9729, 5, 6], [9733, 9, 8], [9745, 4, 2], [9749, 5, 6], [9768, 2, 0], [9770, 5, 0], [9779, 2, 0], [9808, 9, 4], [9839, 2, 7], [9856, 9, 5], [9858, 6, 8], [9905, 3, 7], [9944, 3, 8], [9982, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "# index_label_prediction_list 확인\n",
    "print(index_label_prediction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### index_label_prediction_list 이미지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of index_label_prediction_list =>  285 , false_data_index =>  275\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "false_data_index = np.random.randint(len(index_label_prediction_list))\n",
    "\n",
    "print('len of index_label_prediction_list => ', len(index_label_prediction_list), ', false_data_index => ', false_data_index)\n",
    "\n",
    "mnist_index = index_label_prediction_list[false_data_index][0]\n",
    "label = index_label_prediction_list[false_data_index][1]\n",
    "prediction = index_label_prediction_list[false_data_index][2]\n",
    "\n",
    "title_str = 'index = ' + str(mnist_index) + ' , label = ' + str(label) + ' , prediction = ' + str(prediction)\n",
    "\n",
    "img = test_data[mnist_index, 1:].reshape(28,28)\n",
    "\n",
    "\n",
    "plt.title(title_str)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
